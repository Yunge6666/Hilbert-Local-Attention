Pytorch version should be >= 2.7.0!

attn_reshape.ipynb includes timing tests. After installing the required libraries, you can run it directly on a GPU. Because FlexAttention support varies across different hardware architectures, different GPUs may achieve different acceleration results. Furthermore, due to the volatility of CUDA timing, it is recommended not to rely solely on single measurements.

Pretrained weighs for HNT-tiny and HWT-tiny will be updated later.

More test code and detailed steps will also be updated.
