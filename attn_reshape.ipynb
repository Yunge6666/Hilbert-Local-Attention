{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c9b7b5",
   "metadata": {
    "id": "04c9b7b5"
   },
   "source": [
    "Window-based attention benchmark\n",
    "Include:\n",
    "- Regular Swin Transformer Window Attention\n",
    "- 2D Window Flex Attention\n",
    "- Hilbert Window Attention\n",
    "\n",
    "Sliding Window / Neighborhood attention benchmark\n",
    "Include:\n",
    "- Regular Slide Attention\n",
    "- Regular NATTEN Attention\n",
    "- 2D Flex Sliding Window Attention\n",
    "- Hilbert Sliding Window Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5b9843",
   "metadata": {
    "id": "4d5b9843"
   },
   "source": [
    "# Import library and initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a853b153",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4755,
     "status": "ok",
     "timestamp": 1758422504003,
     "user": {
      "displayName": "Yunge Li",
      "userId": "12625983184086276329"
     },
     "user_tz": 240
    },
    "id": "a853b153",
    "outputId": "4edbed61-9ff5-4a8a-fbdd-5b6e13a65f63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default sparsity block size: 128\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from functools import lru_cache, partial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tabulate import tabulate\n",
    "from torch.nn.attention.flex_attention import (\n",
    "    _DEFAULT_SPARSE_BLOCK_SIZE,\n",
    "    create_block_mask,\n",
    "    create_mask,\n",
    "    flex_attention,\n",
    ")\n",
    "from triton.testing import do_bench\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "torch._dynamo.config.cache_size_limit = 1000\n",
    "\n",
    "# Compile the flex_attention function\n",
    "flex_attention = torch.compile(flex_attention, dynamic=False)\n",
    "\n",
    "data_type = torch.float16\n",
    "\n",
    "# The kernels will utilize block sparisty to increase performance\n",
    "print(f\"Using the default sparsity block size: {_DEFAULT_SPARSE_BLOCK_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zFV-9k4ZMBCD",
   "metadata": {
    "id": "zFV-9k4ZMBCD"
   },
   "source": [
    "# WSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ca4a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1758426541038,
     "user": {
      "displayName": "Yunge Li",
      "userId": "12625983184086276329"
     },
     "user_tz": 240
    },
    "id": "6d4ca4a6",
    "outputId": "996e2666-4b79-4e94-d0de-78dd179adbda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "性能测试结果:\n",
      "窗口重排时间: 0.1978 ms\n",
      "QKV投影时间: 0.9361 ms\n",
      "纯注意力前向: 0.6680 ms\n",
      "端到端前向: 1.5784 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def swin_window_rearrange(x, H_img: int, W_img: int, WINDOW: int):\n",
    "    B, H_img, W_img, C = x.shape\n",
    "    assert H_img % WINDOW == 0 and W_img % WINDOW == 0\n",
    "\n",
    "    num_win_h = H_img // WINDOW\n",
    "    num_win_w = W_img // WINDOW\n",
    "    Lw = WINDOW * WINDOW\n",
    "    Nw = num_win_h * num_win_w\n",
    "\n",
    "    # rearrange the 2D image to window sequence\n",
    "    x_windows = x.view(B, num_win_h, WINDOW, num_win_w, WINDOW, C)\n",
    "    x_windows = x_windows.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "    x_windows = x_windows.view(B * Nw, Lw, C)\n",
    "\n",
    "    shape_info = (B, C, H_img, W_img, num_win_h, num_win_w, Nw, Lw)\n",
    "    return x_windows, shape_info\n",
    "\n",
    "def swin_qkv_projection(x_windows, num_heads: int):\n",
    "\n",
    "    B_Nw, Lw, C = x_windows.shape\n",
    "    head_dim = C // num_heads\n",
    "\n",
    "    # QKV projection - create fixed weight matrix\n",
    "    if not hasattr(swin_qkv_projection, 'qkv_weight'):\n",
    "        swin_qkv_projection.qkv_weight = torch.randn(\n",
    "            C, 3*C, device=x_windows.device, dtype=x_windows.dtype\n",
    "        )\n",
    "\n",
    "    # QKV projection (self.qkv(x) in SWIN)\n",
    "    qkv = x_windows @ swin_qkv_projection.qkv_weight\n",
    "    qkv = qkv.reshape(B_Nw, Lw, 3, num_heads, head_dim).permute(2, 0, 3, 1, 4).contiguous()\n",
    "    q, k, v = qkv[0], qkv[1], qkv[2]  # shape: (B*Nw, num_heads, Lw, head_dim)\n",
    "\n",
    "    return q, k, v\n",
    "\n",
    "def restore_windows(windows, shape_info):\n",
    "    B, C, H_img, W_img, num_win_h, num_win_w, Nw, Lw = shape_info\n",
    "    WINDOW = int(math.sqrt(Lw))\n",
    "\n",
    "    if len(windows.shape) == 4:  # (B*Nw, num_heads, Lw, head_dim)\n",
    "        B_Nw, num_heads, Lw, head_dim = windows.shape\n",
    "        windows = windows.transpose(1, 2).reshape(B_Nw, Lw, num_heads * head_dim)\n",
    "    x = windows.view(B, num_win_h, num_win_w, WINDOW, WINDOW, C)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "    x = x.view(B, H_img, W_img, C)\n",
    "    x = x.view(B, H_img * W_img, C)\n",
    "    return x\n",
    "\n",
    "def swin_attention_only(q, k, v, rpb=None):\n",
    "    \"\"\"Pure Swin attention calculation, without QKV projection\"\"\"\n",
    "    B_Nw, num_heads, Lw, head_dim = q.shape\n",
    "\n",
    "    q = q * (head_dim ** -0.5)\n",
    "    attn = q @ k.transpose(-2, -1)  # (B*Nw, num_heads, Lw, Lw)\n",
    "    if rpb is not None:\n",
    "        attn = attn + rpb.unsqueeze(0)\n",
    "    attn = attn.softmax(dim=-1)\n",
    "\n",
    "    out = attn @ v  # (B*Nw, num_heads, Lw, head_dim)\n",
    "    out = out.transpose(1, 2).reshape(B_Nw, Lw, num_heads * head_dim)  # (B*Nw, Lw, C)\n",
    "    # out = F.scaled_dot_product_attention(\n",
    "    # q, k, v,\n",
    "    # attn_mask=rpb,            # ← additive bias (RPB)\n",
    "    # dropout_p=0.0,\n",
    "    # is_causal=False\n",
    "    # )\n",
    "    return out\n",
    "\n",
    "def create_simple_rpb(Lw, num_heads, device, dtype):\n",
    "    \"\"\"Create a simple 1D global RPB outside\"\"\"\n",
    "    # directly create the bias matrix (num_heads, Lw, Lw)\n",
    "    rpb = torch.randn(num_heads, Lw, Lw, device=device, dtype=dtype) * 0.02\n",
    "    return rpb\n",
    "\n",
    "def test():\n",
    "    B, H_img, W_img, C, WINDOW = 16, 128, 128, 128, 16\n",
    "    S = H_img * W_img\n",
    "    num_heads = 2\n",
    "    device = \"cuda\"\n",
    "    dtype = torch.float16\n",
    "\n",
    "    # prepare the input and gradient\n",
    "    x = torch.randn(B, H_img, W_img, C, device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "    gradOut = torch.randn(B, H_img, W_img, C, device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "    # test the window rearrange and QKV projection\n",
    "    x_windows, shape_info = swin_window_rearrange(x, H_img, W_img, WINDOW)\n",
    "    q, k, v = swin_qkv_projection(x_windows, num_heads)\n",
    "\n",
    "    # prepare the gradient of the windowed\n",
    "    gradOut_windows = gradOut.view(B, H_img, W_img, C)\n",
    "    gradOut_windows = gradOut_windows.view(B, H_img//WINDOW, WINDOW, W_img//WINDOW, WINDOW, C)\n",
    "    gradOut_windows = gradOut_windows.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "    gradOut_windows = gradOut_windows.view(B * (H_img//WINDOW) * (W_img//WINDOW), WINDOW*WINDOW, C)\n",
    "\n",
    "    Lw = WINDOW * WINDOW\n",
    "    rpb = create_simple_rpb(Lw, num_heads, device, dtype)\n",
    "\n",
    "    # Forward - window rearrange time\n",
    "    window_rearrange_call = lambda: swin_window_rearrange(x, H_img, W_img, WINDOW)\n",
    "    window_rearrange_fw = do_bench(window_rearrange_call)\n",
    "\n",
    "    # Forward - QKV projection time\n",
    "    qkv_proj_call = lambda: swin_qkv_projection(x_windows, num_heads)\n",
    "    qkv_proj_fw = do_bench(qkv_proj_call)\n",
    "\n",
    "    # Forward - pure attention (without QKV projection and rearrange)\n",
    "    pure_attention_call = lambda: swin_attention_only(q, k, v, rpb=rpb)\n",
    "    pure_fw = do_bench(pure_attention_call)\n",
    "\n",
    "    # Forward - end-to-end (with rearrange+QKV+attention+restore)\n",
    "    def e2e_call():\n",
    "        x_windows_tmp, shape_tmp = swin_window_rearrange(x, H_img, W_img, WINDOW)\n",
    "        q_tmp, k_tmp, v_tmp = swin_qkv_projection(x_windows_tmp, num_heads)\n",
    "        out_win = swin_attention_only(q_tmp, k_tmp, v_tmp, rpb=rpb)\n",
    "        # out = restore_windows(out_win, shape_tmp)\n",
    "        return out_win\n",
    "    e2e_fw = do_bench(e2e_call)\n",
    "\n",
    "\n",
    "    print(f\"\\nPerformance test results:\")\n",
    "    print(f\"Window rearrange time: {window_rearrange_fw:.4f} ms\")\n",
    "    print(f\"QKV projection time: {qkv_proj_fw:.4f} ms\")\n",
    "    print(f\"Pure attention forward: {pure_fw:.4f} ms\")\n",
    "    print(f\"End-to-end forward: {e2e_fw:.4f} ms\")\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UqcChtm5M1fh",
   "metadata": {
    "id": "UqcChtm5M1fh"
   },
   "source": [
    "# HWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87884774",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 8603,
     "status": "error",
     "timestamp": 1758427197680,
     "user": {
      "displayName": "Yunge Li",
      "userId": "12625983184086276329"
     },
     "user_tz": 240
    },
    "id": "87884774",
    "outputId": "5e028be9-ec59-4f30-e599-effab5447be5"
   },
   "outputs": [],
   "source": [
    "def sgn(x):\n",
    "    return -1 if x < 0 else (1 if x > 0 else 0)\n",
    "\n",
    "def generate2d(x: int, y: int, ax: int, ay: int, bx: int, by: int, result):\n",
    "    w = abs(ax + ay)\n",
    "    h = abs(bx + by)\n",
    "    dax, day = sgn(ax), sgn(ay)\n",
    "    dbx, dby = sgn(bx), sgn(by)\n",
    "\n",
    "    if h == 1 or w == 1:\n",
    "        if h == 1:\n",
    "            for _ in range(w):\n",
    "                result.append((x, y))\n",
    "                x, y = x + dax, y + day\n",
    "        elif w == 1:\n",
    "            for _ in range(h):\n",
    "                result.append((x, y))\n",
    "                x, y = x + dbx, y + dby\n",
    "        return\n",
    "\n",
    "    ax2, ay2 = ax // 2, ay // 2\n",
    "    bx2, by2 = bx // 2, by // 2\n",
    "    w2 = abs(ax2 + ay2)\n",
    "    h2 = abs(bx2 + by2)\n",
    "\n",
    "    if 2 * w > 3 * h:\n",
    "        if w2 % 2 and w > 2:\n",
    "            ax2, ay2 = ax2 + dax, ay2 + day\n",
    "        generate2d(x, y, ax2, ay2, bx, by, result)\n",
    "        generate2d(x + ax2, y + ay2, ax - ax2, ay - ay2, bx, by, result)\n",
    "    else:\n",
    "        if h2 % 2 and h > 2:\n",
    "            bx2, by2 = bx2 + dbx, by2 + dby\n",
    "        generate2d(x, y, bx2, by2, ax2, ay2, result)\n",
    "        generate2d(x + bx2, y + by2, ax, ay, bx - bx2, by - by2, result)\n",
    "        generate2d(x + (ax - dax) + (bx2 - dbx),\n",
    "                   y + (ay - day) + (by2 - dby),\n",
    "                   -bx2, -by2, -(ax - ax2), -(ay - ay2), result)\n",
    "\n",
    "def gilbert2d(width, height):\n",
    "    result = []\n",
    "    if width >= height:\n",
    "        generate2d(0, 0, width, 0, 0, height, result)\n",
    "    else:\n",
    "        generate2d(0, 0, 0, height, width, 0, result)\n",
    "    return result\n",
    "\n",
    "class GilbertPathCache:\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.device_index_cache = {}\n",
    "\n",
    "    def get_or_create_path(self, H, W):\n",
    "        key = (H, W)\n",
    "        if key not in self.cache:\n",
    "            path = gilbert2d(W, H)\n",
    "\n",
    "            forward_map = torch.zeros((H, W), dtype=torch.long)\n",
    "            reverse_map = torch.zeros((H * W, 2), dtype=torch.long)\n",
    "\n",
    "            for idx, (x, y) in enumerate(path[:H*W]):\n",
    "                if y < H and x < W:\n",
    "                    forward_map[y, x] = idx\n",
    "                    reverse_map[idx, 0] = y\n",
    "                    reverse_map[idx, 1] = x\n",
    "\n",
    "            self.cache[key] = {\n",
    "                'path': path,\n",
    "                'forward_map': forward_map,\n",
    "                'reverse_map': reverse_map,\n",
    "                'y_indices': reverse_map[:, 0].clone(),\n",
    "                'x_indices': reverse_map[:, 1].clone(),\n",
    "                'H': H,\n",
    "                'W': W\n",
    "            }\n",
    "\n",
    "        return self.cache[key]\n",
    "\n",
    "    def get_indices_on_device(self, H, W, device):\n",
    "        device_key = (H, W, str(device))\n",
    "        if device_key in self.device_index_cache:\n",
    "            return self.device_index_cache[device_key]\n",
    "        info = self.get_or_create_path(H, W)\n",
    "        y_dev = info['y_indices'].to(device)\n",
    "        x_dev = info['x_indices'].to(device)\n",
    "        self.device_index_cache[device_key] = (y_dev, x_dev)\n",
    "        return y_dev, x_dev\n",
    "\n",
    "    def precompute_paths(self, resolutions):\n",
    "        for H, W in resolutions:\n",
    "            self.get_or_create_path(H, W)\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self.cache.clear()\n",
    "\n",
    "_global_gilbert_cache = GilbertPathCache()\n",
    "\n",
    "def tensor_to_gilbert_path(x, cache=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: Input tensor, shape (B, H, W, C)\n",
    "        cache: Optional GilbertPathCache instance, use global cache if None\n",
    "    Returns:\n",
    "        Reordered tensor, shape (B, H*W, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    device = x.device\n",
    "    if cache is None:\n",
    "        cache = _global_gilbert_cache\n",
    "\n",
    "    y_indices, x_indices = cache.get_indices_on_device(H, W, device)\n",
    "    gilbert_tensor = x[:, y_indices, x_indices, :]  # (B, H*W, C)\n",
    "\n",
    "    return gilbert_tensor\n",
    "\n",
    "def gilbert_tensor_to_2d(x, H, W, cache=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: Gilbert sequence tensor, shape (B, H*W, C)\n",
    "        H: Target height\n",
    "        W: Target width\n",
    "        cache: Optional GilbertPathCache instance, use global cache if None\n",
    "    Returns:\n",
    "        2D layout tensor, shape (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B, N, C = x.shape\n",
    "    device = x.device\n",
    "\n",
    "    if cache is None:\n",
    "        cache = _global_gilbert_cache\n",
    "\n",
    "    output_2d = torch.zeros((B, H, W, C), dtype=x.dtype, device=device)\n",
    "\n",
    "    valid_n = min(N, H * W)\n",
    "    if valid_n > 0:\n",
    "        y_all, x_all = cache.get_indices_on_device(H, W, device)\n",
    "        y_indices = y_all[:valid_n]\n",
    "        x_indices = x_all[:valid_n]\n",
    "\n",
    "        output_2d[:, y_indices, x_indices, :] = x[:, :valid_n, :]\n",
    "\n",
    "    return output_2d\n",
    "\n",
    "@lru_cache\n",
    "def create_block_mask_cached(score_mod, B, H, M, N, device=\"cuda\",BLOCK_SIZE=128):\n",
    "    block_mask = create_block_mask(score_mod, B, H, M, N, device=device,BLOCK_SIZE=BLOCK_SIZE, _compile=True)\n",
    "    return block_mask\n",
    "\n",
    "def calculate_tflops(flops: float, time_ms: float, multiplier: int) -> float:\n",
    "    return multiplier * flops * (1e3 / time_ms) / 1e12\n",
    "\n",
    "def hilbert_rearrange(x):\n",
    "    x_seq = tensor_to_gilbert_path(x)  # (B, H_img*W_img, C)\n",
    "    return x_seq\n",
    "\n",
    "def hilbert_qkv_projection(x_seq, num_heads: int):\n",
    "    B, S, C = x_seq.shape\n",
    "    head_dim = C // num_heads\n",
    "\n",
    "    if not hasattr(hilbert_qkv_projection, 'qkv_weight'):\n",
    "        hilbert_qkv_projection.qkv_weight = torch.randn(\n",
    "            C, 3*C, device=x_seq.device, dtype=x_seq.dtype\n",
    "        )\n",
    "\n",
    "    qkv = x_seq @ hilbert_qkv_projection.qkv_weight  # (B, H_img*W_img, 3*C)\n",
    "    qkv = qkv.view(B, S, 3, num_heads, head_dim).permute(2, 0, 3, 1, 4)\n",
    "    q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "    return q, k, v\n",
    "\n",
    "def prepare_hilbert_qkv(x, num_heads):\n",
    "    # Hilbert reorder\n",
    "    x_seq = hilbert_rearrange(x)\n",
    "\n",
    "    # QKV projection and reshape\n",
    "    q, k, v = hilbert_qkv_projection(x_seq, num_heads)\n",
    "\n",
    "    return q, k, v\n",
    "\n",
    "def hilbert_flex_attention_only(q, k, v, score_mod=None, block_mask=None):\n",
    "\n",
    "    return flex_attention(q, k, v, score_mod=score_mod, block_mask=block_mask)\n",
    "\n",
    "\n",
    "def hilbert_window_flex_attention(x, num_heads, score_mod=None, block_mask=None):\n",
    "\n",
    "    x_seq = hilbert_rearrange(x)\n",
    "    q, k, v = hilbert_qkv_projection(x_seq, num_heads)\n",
    "    x_seq = hilbert_flex_attention_only(q, k, v, score_mod=score_mod, block_mask=block_mask)\n",
    "    return x_seq\n",
    "\n",
    "# Test performance of split functions\n",
    "def test_split_performance():\n",
    "    B, H_img, W_img, C, num_heads = 16, 64, 64, 128, 2\n",
    "    BLOCK = 64  # Block size\n",
    "    S = H_img * W_img\n",
    "\n",
    "    # Prepare input\n",
    "    x = torch.randn(B, H_img, W_img, C, device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "    gradOut = torch.randn(B, H_img, W_img, C, device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "    # Create block mask\n",
    "    def block_window_1d(b, h, q_idx, kv_idx):\n",
    "        return (q_idx // BLOCK) == (kv_idx // BLOCK)\n",
    "    def score_mod_func(score, b, h, q_idx, kv_idx):\n",
    "        rel_pos = (q_idx - kv_idx).to(score.dtype)\n",
    "        return score + rel_pos\n",
    "\n",
    "    block_mask = create_block_mask_cached(block_window_1d, 1, 1, S, S, device=x.device, BLOCK_SIZE=128)\n",
    "\n",
    "    # Test Hilbert rearrangement only\n",
    "    hilbert_rearrange_call = lambda: hilbert_rearrange(x)\n",
    "    hilbert_rearrange_ms = do_bench(hilbert_rearrange_call)\n",
    "\n",
    "    # Test QKV projection only\n",
    "    x_seq = hilbert_rearrange_call()\n",
    "    qkv_proj_call = lambda: hilbert_qkv_projection(x_seq, num_heads)\n",
    "    qkv_proj_ms = do_bench(qkv_proj_call)\n",
    "\n",
    "    # Test QKV preparation (combined)\n",
    "    prepare_call = lambda: prepare_hilbert_qkv(x, num_heads)\n",
    "    prepare_ms = do_bench(prepare_call)\n",
    "\n",
    "    # Pre-compute QKV for pure attention test\n",
    "    q, k, v = prepare_call()\n",
    "\n",
    "    # Test pure flex attention only\n",
    "    pure_attention_call = lambda: hilbert_flex_attention_only(q, k, v, score_mod=score_mod_func, block_mask=block_mask)\n",
    "    pure_attention_ms = do_bench(pure_attention_call)\n",
    "\n",
    "    # Test combined function\n",
    "    combined_call = lambda: hilbert_window_flex_attention(x, num_heads, score_mod=score_mod_func, block_mask=block_mask)\n",
    "    combined_ms = do_bench(combined_call)\n",
    "\n",
    "    # Backward test\n",
    "    combined_out = combined_call()\n",
    "    pure_out = pure_attention_call()\n",
    "    gradOut_seq = gradOut.view(B, H_img*W_img, num_heads, C // num_heads).permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "    pure_bw_ms = do_bench(lambda: pure_out.backward(gradOut_seq, retain_graph=True))\n",
    "    combined_bw_ms = do_bench(lambda: combined_out.backward(gradOut_seq, retain_graph=True))\n",
    "\n",
    "    results = [\n",
    "        [\"Hilbert Rearrangement\", f\"{hilbert_rearrange_ms:.4f}\", \"-\", \"-\", \"-\"],\n",
    "        [\"QKV Projection\", f\"{qkv_proj_ms:.4f}\", \"-\", \"-\", \"-\"],\n",
    "        [\"QKV Preparation (Total)\", f\"{prepare_ms:.4f}\", \"-\", \"-\", \"-\"],\n",
    "        [\"Pure Flex Attention\", f\"{pure_attention_ms:.4f}\", \"-\", f\"{pure_bw_ms:.4f}\", \"-\"],\n",
    "        [\"Combined (Total)\", f\"{combined_ms:.4f}\", \"-\", f\"{combined_bw_ms:.4f}\", \"-\"],\n",
    "        [\"Overhead\", f\"{combined_ms - pure_attention_ms:.4f}\", \"-\", f\"{combined_bw_ms - pure_bw_ms:.4f}\", \"-\"],\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nFunction performance test:\")\n",
    "    print(tabulate(results, headers=[\"Operation\", \"FW Time (ms)\", \"FW FLOPS (TF/s)\", \"BW Time (ms)\", \"BW FLOPS (TF/s)\"], tablefmt=\"grid\"))\n",
    "\n",
    "    # Clean up\n",
    "    del x, q, k, v, combined_out, pure_out\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "test_split_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4d1007",
   "metadata": {
    "id": "1a4d1007"
   },
   "source": [
    "# FlexWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e2e6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3023,
     "status": "ok",
     "timestamp": 1758419007212,
     "user": {
      "displayName": "Yunge Li",
      "userId": "12625983184086276329"
     },
     "user_tz": 240
    },
    "id": "164e2e6a",
    "outputId": "d745b7e6-bcd8-4c9c-f634-365adea62fb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分离函数性能测试:\n",
      "+---------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Operation           |   FW Time (ms) | FW FLOPS (TF/s)   | BW Time (ms)   | BW FLOPS (TF/s)   |\n",
      "+=====================+================+===================+================+===================+\n",
      "| QKV Preparation     |         0.3212 | -                 | -              | -                 |\n",
      "+---------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Pure Flex Attention |         2.6972 | -                 | 13.1964        | -                 |\n",
      "+---------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Combined (Total)    |         2.9507 | -                 | 13.2315        | -                 |\n",
      "+---------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Overhead            |         0.2535 | -                 | 0.0351         | -                 |\n",
      "+---------------------+----------------+-------------------+----------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "@lru_cache\n",
    "def create_block_mask_cached(score_mod, B, H, M, N, device=\"cuda\",BLOCK_SIZE=128):\n",
    "    block_mask = create_block_mask(score_mod, B, H, M, N, device=device,BLOCK_SIZE=BLOCK_SIZE, _compile=True)\n",
    "    return block_mask\n",
    "\n",
    "def calculate_tflops(flops: float, time_ms: float, multiplier: int) -> float:\n",
    "    return multiplier * flops * (1e3 / time_ms) / 1e12\n",
    "\n",
    "def prepare_hilbert_qkv(x, num_heads):\n",
    "\n",
    "    B, H_img, W_img, C = x.shape\n",
    "    S = H_img * W_img\n",
    "    x_seq = x.view(B, S, C)  # Flatten the input\n",
    "\n",
    "    # Add QKV projection\n",
    "    if not hasattr(prepare_hilbert_qkv, 'qkv_weight'):\n",
    "        prepare_hilbert_qkv.qkv_weight = torch.randn(\n",
    "            C, 3*C, device=x_seq.device, dtype=x_seq.dtype\n",
    "        )\n",
    "\n",
    "    # QKV projection\n",
    "    qkv = x_seq @ prepare_hilbert_qkv.qkv_weight  # (B, S, 3*C)\n",
    "    qkv = qkv.view(B, S, 3, num_heads, C // num_heads).permute(2, 0, 3, 1, 4)\n",
    "    q, k, v = qkv[0], qkv[1], qkv[2]  #shape: (B, num_heads, S, C//num_heads)\n",
    "\n",
    "    return q, k, v\n",
    "\n",
    "def flex_attention_only(q, k, v, score_mod=None, block_mask=None):\n",
    "\n",
    "    return flex_attention(q, k, v, score_mod=score_mod, block_mask=block_mask)\n",
    "\n",
    "def window_flex_attention(x, num_heads, score_mod=None, block_mask=None):\n",
    "\n",
    "    q, k, v = prepare_hilbert_qkv(x, num_heads)\n",
    "    x_seq = flex_attention_only(q, k, v, score_mod=score_mod, block_mask=block_mask)\n",
    "    return x_seq\n",
    "\n",
    "# Test performance of split functions\n",
    "def test_split_performance():\n",
    "    B, H_img, W_img, C, num_heads = 16, 128, 128, 128, 2\n",
    "    S = H_img * W_img\n",
    "\n",
    "    # Prepare input\n",
    "    x = torch.randn(B, H_img, W_img, C, device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "    gradOut = torch.randn(B, H_img, W_img, C, device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "    # Create block mask\n",
    "    WINDOW = 16  # Block size\n",
    "    def get_x_y(idx):\n",
    "        return idx // W_img, idx % W_img \n",
    "\n",
    "    def swin_window_mask(b, h, q_idx, kv_idx):\n",
    "        q_x, q_y = get_x_y(q_idx)\n",
    "        k_x, k_y = get_x_y(kv_idx)\n",
    "        same_win_row = (q_x // WINDOW) == (k_x // WINDOW)\n",
    "        same_win_col = (q_y // WINDOW) == (k_y // WINDOW)\n",
    "        return same_win_row & same_win_col\n",
    "\n",
    "    def score_mod_func(score, b, h, q_idx, kv_idx):\n",
    "        rel_pos = (q_idx - kv_idx).to(score.dtype)\n",
    "        return score + rel_pos\n",
    "    block_mask = create_block_mask_cached(swin_window_mask, 1, 1, S, S, device=x.device, BLOCK_SIZE=128)\n",
    "    # Test QKV preparation only\n",
    "    prepare_call = lambda: prepare_hilbert_qkv(x, num_heads)\n",
    "    prepare_ms = do_bench(prepare_call)\n",
    "\n",
    "    # Pre-compute QKV for pure attention test\n",
    "    q, k, v = prepare_call()\n",
    "\n",
    "    # Test pure flex attention only\n",
    "    pure_attention_call = lambda: flex_attention_only(q, k, v, score_mod=score_mod_func, block_mask=block_mask)\n",
    "    pure_attention_ms = do_bench(pure_attention_call)\n",
    "\n",
    "    # Test combined function\n",
    "    combined_call = lambda: window_flex_attention(x, num_heads,score_mod=score_mod_func, block_mask=block_mask)\n",
    "    combined_ms = do_bench(combined_call)\n",
    "\n",
    "    # Backward test\n",
    "    combined_out = combined_call()\n",
    "    pure_out = pure_attention_call()\n",
    "    gradOut_seq = gradOut.view(B, H_img*W_img, num_heads, C // num_heads).permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "    pure_bw_ms = do_bench(lambda: pure_out.backward(gradOut_seq, retain_graph=True))\n",
    "    combined_bw_ms = do_bench(lambda: combined_out.backward(gradOut_seq, retain_graph=True))\n",
    "\n",
    "    results = [\n",
    "        [\"QKV Preparation\", f\"{prepare_ms:.4f}\", \"-\", \"-\", \"-\"],\n",
    "        [\"Pure Flex Attention\", f\"{pure_attention_ms:.4f}\", \"-\", f\"{pure_bw_ms:.4f}\", \"-\"],\n",
    "        [\"Combined (Total)\", f\"{combined_ms:.4f}\", \"-\", f\"{combined_bw_ms:.4f}\", \"-\"],\n",
    "        [\"Overhead\", f\"{combined_ms - pure_attention_ms:.4f}\", \"-\", f\"{combined_bw_ms - pure_bw_ms:.4f}\", \"-\"],\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nFunction performance test:\")\n",
    "    print(tabulate(results, headers=[\"Operation\", \"FW Time (ms)\", \"FW FLOPS (TF/s)\", \"BW Time (ms)\", \"BW FLOPS (TF/s)\"], tablefmt=\"grid\"))\n",
    "\n",
    "    # Clean up\n",
    "    del x, q, k, v, combined_out, pure_out\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "test_split_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2bd189",
   "metadata": {
    "id": "dc2bd189"
   },
   "source": [
    "Regular Slide Transformer Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55af10",
   "metadata": {
    "id": "8b55af10",
    "outputId": "a641dce2-384f-46ab-effe-653376df3af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SlideAttentionSplit 类已创建，forward方法已拆分为:\n",
      "开始性能测试...\n",
      "完整forward时间: 111.9284 ms\n",
      "预处理时间: 106.5070 ms\n",
      "注意力计算时间: 5.8193 ms\n",
      "预处理+注意力 ≈ 112.3262 ms\n",
      "注意力计算占比: 5.2%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(111.928408203125, 106.506953125, 5.819269409179688)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import trunc_normal_\n",
    "from triton.testing import do_bench\n",
    "\n",
    "class SlideAttentionSplit(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_resolution, dim, num_heads, ka, qkv_bias=True, qk_scale=None,\n",
    "        attn_drop=0., proj_drop=0., padding_mode='zeros'):\n",
    "\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or self.head_dim ** -0.5\n",
    "        self.padding_mode = padding_mode\n",
    "        self.ka = ka\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.dep_conv = nn.Conv2d(self.head_dim, self.ka*self.ka*self.head_dim, kernel_size=self.ka,\n",
    "                                 bias=True, groups=self.head_dim, padding=self.ka//2, padding_mode=padding_mode)\n",
    "        self.dep_conv1 = nn.Conv2d(self.head_dim, self.ka*self.ka*self.head_dim, kernel_size=self.ka,\n",
    "                                  bias=True, groups=self.head_dim, padding=self.ka//2, padding_mode=padding_mode)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "        self.relative_position_bias_table = nn.Parameter(torch.zeros(1, self.num_heads, 1, self.ka*self.ka, 1, 1))\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=3)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # shift initialization for group convolution\n",
    "        kernel = torch.zeros(self.ka*self.ka, self.ka, self.ka)\n",
    "        for i in range(self.ka*self.ka):\n",
    "            kernel[i, i//self.ka, i%self.ka] = 1.\n",
    "        kernel = kernel.unsqueeze(1).repeat(self.head_dim, 1, 1, 1)\n",
    "        self.dep_conv.weight = nn.Parameter(data=kernel, requires_grad=False)\n",
    "\n",
    "    def prepare_qkv(self, x):\n",
    "        B, L, C = x.shape\n",
    "        H, W = self.input_resolution\n",
    "        x = x.view(B, H, W, C)\n",
    "        qkv = self.qkv(x)\n",
    "\n",
    "        f_conv = qkv.permute(0, 3, 1, 2).reshape(B*self.num_heads, 3*self.head_dim, H, W)\n",
    "\n",
    "        q = (f_conv[:, :self.head_dim, :, :] * self.scale).reshape(B, self.num_heads, self.head_dim, 1, H, W)\n",
    "        k = f_conv[:, self.head_dim:2*self.head_dim, :, :]\n",
    "        v = f_conv[:, 2*self.head_dim:, :, :]\n",
    "\n",
    "        k_conv = (self.dep_conv(k) + self.dep_conv1(k))\n",
    "        v_conv = (self.dep_conv(v) + self.dep_conv1(v))\n",
    "\n",
    "        k = k_conv.view(B, self.num_heads, self.ka*self.ka, self.head_dim, H, W).permute(0, 1, 3, 2, 4, 5)\n",
    "        v = v_conv.view(B, self.num_heads, self.ka*self.ka, self.head_dim, H, W).permute(0, 1, 3, 2, 4, 5)\n",
    "\n",
    "        k = k + self.relative_position_bias_table\n",
    "\n",
    "        return q, k, v, (B, L, C)\n",
    "\n",
    "    def compute_attention(self, q, k, v, orig_shape):\n",
    "\n",
    "        B, L, C = orig_shape\n",
    "        H, W = self.input_resolution\n",
    "\n",
    "        attn = (q * k).sum(2, keepdim=True) # B, self.num_heads, 1, ka*ka, H, W\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn * v).sum(3).reshape(B, C, H, W).permute(0, 2, 3, 1).view(B, L, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        q, k, v, orig_shape = self.prepare_qkv(x)\n",
    "        return self.compute_attention(q, k, v, orig_shape)\n",
    "\n",
    "\n",
    "def simple_bench(fn, warmup=25, rep=100):\n",
    "    # warmup\n",
    "    for _ in range(warmup):\n",
    "        fn()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    start.record()\n",
    "    for _ in range(rep):\n",
    "        fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    return start.elapsed_time(end) / rep\n",
    "\n",
    "def test_split_slide_attention():\n",
    "\n",
    "    B, H_img, W_img, dim, num_heads, ka = 16, 56, 56, 128, 2, 7\n",
    "    S = H_img * W_img\n",
    "\n",
    "    slide_attn = SlideAttentionSplit(\n",
    "        input_resolution=(H_img, W_img),\n",
    "        dim=dim,\n",
    "        num_heads=num_heads,\n",
    "        ka=ka\n",
    "    ).cuda().to(torch.float16)\n",
    "\n",
    "    x = torch.randn(B, S, dim, device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "    # test full forward time\n",
    "    fw_fn = lambda: slide_attn(x.clone())\n",
    "    fw_time = simple_bench(fw_fn)\n",
    "\n",
    "    # pre-compute q,k,v for separate attention calculation\n",
    "    with torch.no_grad():\n",
    "        q, k, v, orig_shape = slide_attn.prepare_qkv(x)\n",
    "\n",
    "    # test pre-processing time\n",
    "    prep_fn = lambda: slide_attn.prepare_qkv(x.clone())\n",
    "    prep_time = simple_bench(prep_fn)\n",
    "\n",
    "    attn_fn = lambda: slide_attn.compute_attention(q, k, v, orig_shape)\n",
    "    attn_time = simple_bench(attn_fn)\n",
    "\n",
    "    print(f\"full forward time: {fw_time:.4f} ms\")\n",
    "    print(f\"pre-processing time: {prep_time:.4f} ms\")\n",
    "    print(f\"attention calculation time: {attn_time:.4f} ms\")\n",
    "    print(f\"pre-processing + attention ≈ {prep_time + attn_time:.4f} ms\")\n",
    "\n",
    "    return fw_time, prep_time, attn_time\n",
    "\n",
    "print(\"start testing...\")\n",
    "test_split_slide_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8d026a",
   "metadata": {
    "id": "6c8d026a"
   },
   "source": [
    "# NA2D (NAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50530b6d",
   "metadata": {
    "id": "50530b6d",
    "outputId": "fb415c9a-b67c-40b3-ca6b-4e269643daea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 NATTEN 2D 分离函数性能测试\n",
      "============================================================\n",
      "\n",
      "📊 测试配置:\n",
      "  批量大小: 16\n",
      "  图像尺寸: 96×96\n",
      "  序列长度: 9216\n",
      "  特征维度: 128\n",
      "  注意力头数: 2\n",
      "  每头维度: 64\n",
      "  窗口大小: 17×17\n",
      "\n",
      "⚙️ 开始性能测试...\n",
      "\n",
      "📊 分离函数性能测试结果:\n",
      "+----------------+----------------+--------------+----------------+--------------+\n",
      "| 操作           |   前向时间(ms) | 前向TFLOPS   | 反向时间(ms)   | 反向TFLOPS   |\n",
      "+================+================+==============+================+==============+\n",
      "| QKV准备        |          0.65  | 22.30        | -              | -            |\n",
      "+----------------+----------------+--------------+----------------+--------------+\n",
      "| 纯NATTEN注意力 |          1.27  | 17.18        | 9.953          | 5.48         |\n",
      "+----------------+----------------+--------------+----------------+--------------+\n",
      "| 组合函数(总计) |          1.905 | 20.01        | 9.915          | 7.69         |\n",
      "+----------------+----------------+--------------+----------------+--------------+\n",
      "| 额外开销       |         -0.016 | -            | -0.038         | -            |\n",
      "+----------------+----------------+--------------+----------------+--------------+\n",
      "\n",
      "📈 时间占比分析:\n",
      "  QKV准备占前向时间: 34.1%\n",
      "  NATTEN注意力占前向时间: 66.7%\n",
      "  额外开销占前向时间: -0.8%\n",
      "  稀疏密度: 3.14%\n",
      "\n",
      "🎯 关键性能指标:\n",
      "  NATTEN峰值前向TFLOPS: 17.18\n",
      "  NATTEN峰值反向TFLOPS: 5.48\n",
      "  端到端前向延迟: 1.905ms\n",
      "  端到端反向延迟: 9.915ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from natten import na2d  #install it refer to the NATTEN github\n",
    "from triton.testing import do_bench\n",
    "from tabulate import tabulate\n",
    "from functools import lru_cache\n",
    "\n",
    "def calculate_tflops(flops: float, time_ms: float, multiplier: int) -> float:\n",
    "    \"\"\"计算TFLOPS\"\"\"\n",
    "    return multiplier * flops * (1e3 / time_ms) / 1e12\n",
    "\n",
    "def prepare_natten_qkv(x, num_heads):\n",
    "    \"\"\"Prepare QKV tensors from 2D input for NATTEN\"\"\"\n",
    "    B, H_img, W_img, C = x.shape\n",
    "    head_dim = C // num_heads\n",
    "\n",
    "    if not hasattr(prepare_natten_qkv, 'qkv_weight'):\n",
    "        prepare_natten_qkv.qkv_weight = torch.randn(\n",
    "            C, 3*C, device=x.device, dtype=x.dtype\n",
    "        )\n",
    "\n",
    "    qkv = x @ prepare_natten_qkv.qkv_weight  # (B, H_img, W_img, 3*C)\n",
    "    qkv = qkv.reshape(B, H_img, W_img, 3, num_heads, head_dim)\n",
    "\n",
    "    qkv = qkv.permute(3, 0, 1, 2, 4, 5).contiguous()  # (3, B, H_img, W_img, num_heads, head_dim)\n",
    "    q, k, v = qkv[0], qkv[1], qkv[2]  # [B, H_img, W_img, num_heads, head_dim]\n",
    "\n",
    "    return q, k, v\n",
    "\n",
    "def natten_attention_only(q, k, v, kernel_size=7):\n",
    "    \"\"\"Execute NATTEN 2D attention on pre-processed QKV tensors\n",
    "\n",
    "    Args:\n",
    "        q, k, v: Query, Key, Value tensors, each with shape (B, H_img, W_img, num_heads, head_dim)\n",
    "        kernel_size: Window size for neighborhood attention\n",
    "\n",
    "    Returns:\n",
    "        Output tensor from NATTEN, shape (B, H_img, W_img, num_heads, head_dim)\n",
    "    \"\"\"\n",
    "    return na2d(q, k, v, kernel_size=kernel_size)\n",
    "\n",
    "def natten_2d_combined(x, num_heads, kernel_size=7):\n",
    "    \"\"\"Combined function for QKV preparation and NATTEN attention\"\"\"\n",
    "    q, k, v = prepare_natten_qkv(x, num_heads)\n",
    "    output = natten_attention_only(q, k, v, kernel_size=kernel_size)\n",
    "    return output\n",
    "\n",
    "def test_natten_split_performance():\n",
    "    \"\"\"Test performance of split NATTEN functions\"\"\"\n",
    "\n",
    "    print(\" NATTEN 2D test\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 配置参数\n",
    "    B, H_img, W_img, C, num_heads = 16, 96, 96, 128, 2\n",
    "    kernel_size = 17\n",
    "    S = H_img * W_img\n",
    "    head_dim = C // num_heads\n",
    "\n",
    "    print(f\"\\n test configuration:\")\n",
    "    print(f\"   batch size: {B}\")\n",
    "    print(f\"   image size: {H_img}×{W_img}\")\n",
    "    print(f\"   sequence length: {S}\")\n",
    "    print(f\"   feature dimension: {C}\")\n",
    "    print(f\"   number of attention heads: {num_heads}\")\n",
    "    print(f\"   head dimension: {head_dim}\")\n",
    "    print(f\"   window size: {kernel_size}×{kernel_size}\")\n",
    "\n",
    "    # 准备输入\n",
    "    x = torch.randn(B, H_img, W_img, C, device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "    grad_out = torch.randn(B, H_img, W_img, num_heads, head_dim, device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "    print(f\"\\n start testing...\")\n",
    "\n",
    "    prepare_call = lambda: prepare_natten_qkv(x, num_heads)\n",
    "    prepare_ms = do_bench(prepare_call)\n",
    "\n",
    "    prepare_flops = B * S * C * C * 3  # QKV projection\n",
    "    prepare_tflops = calculate_tflops(prepare_flops, prepare_ms, 2)\n",
    "\n",
    "    q, k, v = prepare_call()\n",
    "\n",
    "    pure_attention_call = lambda: natten_attention_only(q, k, v, kernel_size=kernel_size)\n",
    "    pure_attention_ms = do_bench(pure_attention_call)\n",
    "\n",
    "    # kernel_area = kernel_size * kernel_size\n",
    "    # density = min(kernel_area / S, 1.0)\n",
    "    # attention_flops = density * B * S * num_heads * head_dim * S\n",
    "    # pure_attention_tflops = calculate_tflops(attention_flops, pure_attention_ms, 4)\n",
    "\n",
    "    # ========== test combined function ==========\n",
    "    combined_call = lambda: natten_2d_combined(x, num_heads, kernel_size=kernel_size)\n",
    "    combined_ms = do_bench(combined_call)\n",
    "\n",
    "    # combined_flops = prepare_flops + attention_flops\n",
    "    # combined_tflops = calculate_tflops(combined_flops, combined_ms, 3)\n",
    "\n",
    "    # ========== Backward ==========\n",
    "    # pure attention backward\n",
    "    pure_out = pure_attention_call()\n",
    "    def pure_backward():\n",
    "        out = pure_out.clone().requires_grad_(True)\n",
    "        out.backward(grad_out, retain_graph=True)\n",
    "    pure_bw_ms = do_bench(pure_backward)\n",
    "    # pure_bw_tflops = calculate_tflops(attention_flops, pure_bw_ms, 10)\n",
    "\n",
    "    # combined function backward\n",
    "    combined_out = combined_call()\n",
    "    def combined_backward():\n",
    "        out = combined_out.clone().requires_grad_(True)\n",
    "        out.backward(grad_out, retain_graph=True)\n",
    "    combined_bw_ms = do_bench(combined_backward)\n",
    "    # combined_bw_tflops = calculate_tflops(combined_flops, combined_bw_ms, 6)\n",
    "\n",
    "    overhead_fw_ms = combined_ms - pure_attention_ms - prepare_ms\n",
    "    overhead_bw_ms = combined_bw_ms - pure_bw_ms\n",
    "\n",
    "    results = [\n",
    "        [\"QKV preparation\",\n",
    "         f\"{prepare_ms:.3f}\",\n",
    "         f\"{prepare_tflops:.2f}\",\n",
    "         \"-\",\n",
    "         \"-\"],\n",
    "        [\"pure NATTEN attention\",\n",
    "         f\"{pure_attention_ms:.3f}\",\n",
    "         f\"{pure_bw_ms:.3f}\"],\n",
    "        [\"combined function (total)\",\n",
    "         f\"{combined_ms:.3f}\",\n",
    "         f\"{combined_bw_ms:.3f}\"],\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n split function performance test results:\")\n",
    "    print(tabulate(\n",
    "        results,\n",
    "        headers=[\"operation\", \"forward time (ms)\", \"forward TFLOPS\", \"backward time (ms)\", \"backward TFLOPS\"],\n",
    "        tablefmt=\"grid\"\n",
    "    ))\n",
    "\n",
    "\n",
    "    print(f\"\\n key performance indicators:\")\n",
    "    print(f\"  forward time: {combined_ms:.3f}ms\")\n",
    "    print(f\"  backward time: {combined_bw_ms:.3f}ms\")\n",
    "\n",
    "    # 清理\n",
    "    del x, q, k, v, combined_out, pure_out, grad_out\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 运行测试\n",
    "if __name__ == \"__main__\":\n",
    "    test_natten_split_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc1ed2a",
   "metadata": {
    "id": "9dc1ed2a"
   },
   "source": [
    "# HNA (NAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af6cd8",
   "metadata": {
    "id": "17af6cd8",
    "outputId": "318134f6-eb88-40b5-ce83-e7e5b9fc4a27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�� NATTEN 2D 分离函数性能测试\n",
      "============================================================\n",
      "\n",
      "�� 测试配置:\n",
      "  批量大小: 16\n",
      "  图像尺寸: 96×96\n",
      "  序列长度: 9216\n",
      "  特征维度: 128\n",
      "  注意力头数: 2\n",
      "  每头维度: 64\n",
      "  窗口大小: 289×289\n",
      "\n",
      "⚙️ 开始性能测试...\n",
      "\n",
      "1️⃣ Hilbert重排:\n",
      "   时间: 0.2201ms\n",
      "\n",
      "2️⃣ QKV投影:\n",
      "   时间: 0.6507ms\n",
      "\n",
      "3️⃣ Hilbert序列 + QKV准备:\n",
      "   时间: 0.8679ms\n",
      "\n",
      "4️⃣ 纯NATTEN 1D注意力:\n",
      "   前向时间: 0.7749ms\n",
      "   前向TFLOPS: 28.16\n",
      "   稀疏密度: 3.14%\n",
      "\n",
      "5️⃣ 组合函数 (Hilbert + QKV + NATTEN):\n",
      "   总时间: 1.6144ms\n",
      "   TFLOPS: 13.51\n",
      "\n",
      "📊 性能测试结果:\n",
      "+-----------------+------------+------------+------------+------------+\n",
      "| 操作            |   前向(ms) | 前向TF/s   | 反向(ms)   | 反向TF/s   |\n",
      "+=================+============+============+============+============+\n",
      "| Hilbert重排     |     0.2201 | -          | -          | -          |\n",
      "+-----------------+------------+------------+------------+------------+\n",
      "| QKV投影         |     0.6507 | -          | -          | -          |\n",
      "+-----------------+------------+------------+------------+------------+\n",
      "| Hilbert+QKV准备 |     0.8679 | -          | -          | -          |\n",
      "+-----------------+------------+------------+------------+------------+\n",
      "| 纯NATTEN 1D     |     0.7749 | 28.16      | 6.6319     | 8.22       |\n",
      "+-----------------+------------+------------+------------+------------+\n",
      "| 组合函数        |     1.6144 | 13.51      | 6.5184     | 8.37       |\n",
      "+-----------------+------------+------------+------------+------------+\n",
      "| 额外开销        |    -0.0284 | -          | -0.1135    | -          |\n",
      "+-----------------+------------+------------+------------+------------+\n",
      "\n",
      "📈 时间占比分析:\n",
      "  Hilbert重排: 13.6% (0.2201ms)\n",
      "  QKV投影: 40.3% (0.6507ms)\n",
      "  NATTEN 1D注意力: 48.0% (0.7749ms)\n",
      "  额外开销: -1.8% (-0.0284ms)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from natten import na2d, na1d\n",
    "from triton.testing import do_bench\n",
    "from tabulate import tabulate\n",
    "from functools import lru_cache\n",
    "def sgn(x):\n",
    "    return -1 if x < 0 else (1 if x > 0 else 0)\n",
    "\n",
    "def generate2d(x: int, y: int, ax: int, ay: int, bx: int, by: int, result):\n",
    "    w = abs(ax + ay)\n",
    "    h = abs(bx + by)\n",
    "    dax, day = sgn(ax), sgn(ay)\n",
    "    dbx, dby = sgn(bx), sgn(by)\n",
    "\n",
    "    if h == 1 or w == 1:\n",
    "        if h == 1:\n",
    "            for _ in range(w):\n",
    "                result.append((x, y))\n",
    "                x, y = x + dax, y + day\n",
    "        elif w == 1:\n",
    "            for _ in range(h):\n",
    "                result.append((x, y))\n",
    "                x, y = x + dbx, y + dby\n",
    "        return\n",
    "\n",
    "    ax2, ay2 = ax // 2, ay // 2\n",
    "    bx2, by2 = bx // 2, by // 2\n",
    "    w2 = abs(ax2 + ay2)\n",
    "    h2 = abs(bx2 + by2)\n",
    "\n",
    "    if 2 * w > 3 * h:\n",
    "        if w2 % 2 and w > 2:\n",
    "            ax2, ay2 = ax2 + dax, ay2 + day\n",
    "        generate2d(x, y, ax2, ay2, bx, by, result)\n",
    "        generate2d(x + ax2, y + ay2, ax - ax2, ay - ay2, bx, by, result)\n",
    "    else:\n",
    "        if h2 % 2 and h > 2:\n",
    "            bx2, by2 = bx2 + dbx, by2 + dby\n",
    "        generate2d(x, y, bx2, by2, ax2, ay2, result)\n",
    "        generate2d(x + bx2, y + by2, ax, ay, bx - bx2, by - by2, result)\n",
    "        generate2d(x + (ax - dax) + (bx2 - dbx),\n",
    "                   y + (ay - day) + (by2 - dby),\n",
    "                   -bx2, -by2, -(ax - ax2), -(ay - ay2), result)\n",
    "\n",
    "def gilbert2d(width, height):\n",
    "    result = []\n",
    "    if width >= height:\n",
    "        generate2d(0, 0, width, 0, 0, height, result)\n",
    "    else:\n",
    "        generate2d(0, 0, 0, height, width, 0, result)\n",
    "    return result\n",
    "\n",
    "class GilbertPathCache:\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.device_index_cache = {}\n",
    "\n",
    "    def get_or_create_path(self, H, W):\n",
    "        key = (H, W)\n",
    "        if key not in self.cache:\n",
    "            path = gilbert2d(W, H)\n",
    "\n",
    "            forward_map = torch.zeros((H, W), dtype=torch.long)\n",
    "            reverse_map = torch.zeros((H * W, 2), dtype=torch.long)\n",
    "\n",
    "            for idx, (x, y) in enumerate(path[:H*W]):\n",
    "                if y < H and x < W:\n",
    "                    forward_map[y, x] = idx\n",
    "                    reverse_map[idx, 0] = y\n",
    "                    reverse_map[idx, 1] = x\n",
    "\n",
    "            self.cache[key] = {\n",
    "                'path': path,\n",
    "                'forward_map': forward_map,\n",
    "                'reverse_map': reverse_map,\n",
    "                'y_indices': reverse_map[:, 0].clone(),\n",
    "                'x_indices': reverse_map[:, 1].clone(),\n",
    "                'H': H,\n",
    "                'W': W\n",
    "            }\n",
    "\n",
    "        return self.cache[key]\n",
    "\n",
    "    def get_indices_on_device(self, H, W, device):\n",
    "        device_key = (H, W, str(device))\n",
    "        if device_key in self.device_index_cache:\n",
    "            return self.device_index_cache[device_key]\n",
    "        info = self.get_or_create_path(H, W)\n",
    "        y_dev = info['y_indices'].to(device)\n",
    "        x_dev = info['x_indices'].to(device)\n",
    "        self.device_index_cache[device_key] = (y_dev, x_dev)\n",
    "        return y_dev, x_dev\n",
    "\n",
    "    def precompute_paths(self, resolutions):\n",
    "        for H, W in resolutions:\n",
    "            self.get_or_create_path(H, W)\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self.cache.clear()\n",
    "\n",
    "_global_gilbert_cache = GilbertPathCache()\n",
    "\n",
    "def tensor_to_gilbert_path(x, cache=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: Input tensor, shape (B, H, W, C)\n",
    "        cache: Optional GilbertPathCache instance, use global cache if None\n",
    "    Returns:\n",
    "        Reordered tensor, shape (B, H*W, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    device = x.device\n",
    "    if cache is None:\n",
    "        cache = _global_gilbert_cache\n",
    "\n",
    "    y_indices, x_indices = cache.get_indices_on_device(H, W, device)\n",
    "    gilbert_tensor = x[:, y_indices, x_indices, :]  # (B, H*W, C)\n",
    "\n",
    "    return gilbert_tensor\n",
    "\n",
    "def gilbert_tensor_to_2d(x, H, W, cache=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: Gilbert sequence tensor, shape (B, H*W, C)\n",
    "        H: Target height\n",
    "        W: Target width\n",
    "        cache: Optional GilbertPathCache instance, use global cache if None\n",
    "    Returns:\n",
    "        2D layout tensor, shape (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B, N, C = x.shape\n",
    "    device = x.device\n",
    "\n",
    "    if cache is None:\n",
    "        cache = _global_gilbert_cache\n",
    "\n",
    "    output_2d = torch.zeros((B, H, W, C), dtype=x.dtype, device=device)\n",
    "\n",
    "    valid_n = min(N, H * W)\n",
    "    if valid_n > 0:\n",
    "        y_all, x_all = cache.get_indices_on_device(H, W, device)\n",
    "        y_indices = y_all[:valid_n]\n",
    "        x_indices = x_all[:valid_n]\n",
    "\n",
    "        output_2d[:, y_indices, x_indices, :] = x[:, :valid_n, :]\n",
    "\n",
    "    return output_2d\n",
    "def calculate_tflops(flops: float, time_ms: float, multiplier: int) -> float:\n",
    "    \"\"\"计算TFLOPS\"\"\"\n",
    "    return multiplier * flops * (1e3 / time_ms) / 1e12\n",
    "\n",
    "def natten_hilbert_rearrange(x):\n",
    "\n",
    "    x_seq = tensor_to_gilbert_path(x)  # (B, H_img*W_img, C)\n",
    "    return x_seq\n",
    "\n",
    "def natten_qkv_projection(x_seq, num_heads: int):\n",
    "\n",
    "    B, S, C = x_seq.shape\n",
    "    head_dim = C // num_heads\n",
    "\n",
    "    if not hasattr(natten_qkv_projection, 'qkv_weight'):\n",
    "        natten_qkv_projection.qkv_weight = torch.randn(\n",
    "            C, 3*C, device=x_seq.device, dtype=x_seq.dtype\n",
    "        )\n",
    "\n",
    "    # QKV projection\n",
    "    qkv = x_seq @ natten_qkv_projection.qkv_weight  # (B, S, 3*C)\n",
    "    qkv = qkv.view(B, S, 3, num_heads, head_dim).permute(2, 0, 1, 3, 4).contiguous()  # (3, B, S, H, D)\n",
    "    q, k, v = qkv[0], qkv[1], qkv[2]  \n",
    "\n",
    "    return q, k, v\n",
    "\n",
    "def prepare_natten_qkv_with_hilbert(x, num_heads):\n",
    "\n",
    "    x_seq = natten_hilbert_rearrange(x)\n",
    "\n",
    "    q, k, v = natten_qkv_projection(x_seq, num_heads)\n",
    "\n",
    "    return q, k, v\n",
    "\n",
    "def natten_attention_only(q, k, v, kernel_size=7):\n",
    "\n",
    "    return na1d(q, k, v, kernel_size=kernel_size)\n",
    "\n",
    "def natten_1d_combined(x, num_heads, kernel_size=7):\n",
    "\n",
    "    x_seq = natten_hilbert_rearrange(x)\n",
    "    q, k, v = natten_qkv_projection(x_seq, num_heads)\n",
    "    output = natten_attention_only(q, k, v, kernel_size=kernel_size)\n",
    "    return output\n",
    "\n",
    "def test_natten_split_performance():\n",
    "    \"\"\"Test performance of split NATTEN functions\"\"\"\n",
    "\n",
    "    # 配置参数\n",
    "    B, H_img, W_img, C, num_heads = 16, 96, 96, 128, 2\n",
    "    kernel_size = 289\n",
    "    S = H_img * W_img\n",
    "    head_dim = C // num_heads\n",
    "\n",
    "    print(f\"\\n Test configuration:\")\n",
    "    print(f\"   batch size: {B}\")\n",
    "    print(f\"   image size: {H_img}×{W_img}\")\n",
    "    print(f\"   sequence length: {S}\")\n",
    "    print(f\"   feature dimension: {C}\")\n",
    "    print(f\"   number of attention heads: {num_heads}\")\n",
    "    print(f\"   head dimension: {head_dim}\")\n",
    "    print(f\"   window size: {kernel_size}×{kernel_size}\")\n",
    "\n",
    "    # 准备输入\n",
    "    x = torch.randn(B, H_img, W_img, C, device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "    grad_out = torch.randn(B, S, num_heads, head_dim, device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "    print(f\"\\n start testing...\")\n",
    "\n",
    "    hilbert_rearrange_call = lambda: natten_hilbert_rearrange(x)\n",
    "    hilbert_rearrange_ms = do_bench(hilbert_rearrange_call)\n",
    "\n",
    "\n",
    "    x_seq = hilbert_rearrange_call()\n",
    "    qkv_proj_call = lambda: natten_qkv_projection(x_seq, num_heads)\n",
    "    qkv_proj_ms = do_bench(qkv_proj_call)\n",
    "\n",
    "\n",
    "    prepare_call = lambda: prepare_natten_qkv_with_hilbert(x, num_heads)\n",
    "    prepare_ms = do_bench(prepare_call)\n",
    "\n",
    "    print(f\"\\n Hilbert reorder:\")\n",
    "    print(f\"   Time: {hilbert_rearrange_ms:.4f}ms\")\n",
    "    print(f\"\\n QKV projection:\")\n",
    "    print(f\"   Time: {qkv_proj_ms:.4f}ms\")\n",
    "    print(f\"\\n Hilbert sequence + QKV preparation:\")\n",
    "    print(f\"   Time: {prepare_ms:.4f}ms\")\n",
    "\n",
    "    q, k, v = prepare_call()\n",
    "\n",
    "    pure_attention_call = lambda: natten_attention_only(q, k, v, kernel_size=kernel_size)\n",
    "    pure_attention_ms = do_bench(pure_attention_call)\n",
    "\n",
    "\n",
    "    print(f\"\\n Pure NATTEN 1D attention:\")\n",
    "    print(f\"   Forward time: {pure_attention_ms:.4f}ms\")\n",
    "\n",
    "    combined_call = lambda: natten_1d_combined(x, num_heads, kernel_size=kernel_size)\n",
    "    combined_ms = do_bench(combined_call)\n",
    "\n",
    "    print(f\"\\n Combined function (Hilbert + QKV + NATTEN):\")\n",
    "    print(f\"   Total time: {combined_ms:.4f}ms\")\n",
    "\n",
    "    pure_out = pure_attention_call()\n",
    "    def pure_backward():\n",
    "        out = pure_out.clone().requires_grad_(True)\n",
    "        out.backward(grad_out, retain_graph=True)\n",
    "    pure_bw_ms = do_bench(pure_backward)\n",
    "\n",
    "    combined_out = combined_call()\n",
    "    def combined_backward():\n",
    "        out = combined_out.clone().requires_grad_(True)\n",
    "        out.backward(grad_out, retain_graph=True)\n",
    "    combined_bw_ms = do_bench(combined_backward)\n",
    "\n",
    "    results = [\n",
    "        [\"Hilbert reorder\",\n",
    "         f\"{hilbert_rearrange_ms:.4f}\",\n",
    "         \"-\",\n",
    "         \"-\",\n",
    "         \"-\"],\n",
    "        [\"QKV projection\",\n",
    "         f\"{qkv_proj_ms:.4f}\",\n",
    "         \"-\",\n",
    "         \"-\",\n",
    "         \"-\"],\n",
    "        [\"Hilbert+QKV preparation\",\n",
    "         f\"{prepare_ms:.4f}\",\n",
    "         \"-\",\n",
    "         \"-\",\n",
    "         \"-\"],\n",
    "        [\"Pure NATTEN 1D\",\n",
    "         f\"{pure_attention_ms:.4f}\",\n",
    "         f\"{pure_bw_ms:.4f}\",],\n",
    "        [\"Combined function\",\n",
    "         f\"{combined_ms:.4f}\",\n",
    "         f\"{combined_bw_ms:.4f}\",],\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n Performance test results:\")\n",
    "    print(tabulate(\n",
    "        results,\n",
    "        headers=[\"Operation\", \"Forward (ms)\", \"Backward (ms)\"],\n",
    "        tablefmt=\"grid\"\n",
    "    ))\n",
    "\n",
    "\n",
    "    # clear memory  \n",
    "    del x, q, k, v, combined_out, pure_out, grad_out\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_natten_split_performance()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabbe618",
   "metadata": {
    "id": "eabbe618"
   },
   "source": [
    "# HSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f2c1e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3486,
     "status": "ok",
     "timestamp": 1758422585271,
     "user": {
      "displayName": "Yunge Li",
      "userId": "12625983184086276329"
     },
     "user_tz": 240
    },
    "id": "a9f2c1e1",
    "outputId": "14defcf6-e869-46e6-cf4a-da9238bb0199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分离函数性能测试:\n",
      "+-------------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Operation               |   FW Time (ms) | FW FLOPS (TF/s)   | BW Time (ms)   | BW FLOPS (TF/s)   |\n",
      "+=========================+================+===================+================+===================+\n",
      "| Hilbert Rearrangement   |         0.2395 | -                 | -              | -                 |\n",
      "+-------------------------+----------------+-------------------+----------------+-------------------+\n",
      "| QKV Projection          |         0.1892 | -                 | -              | -                 |\n",
      "+-------------------------+----------------+-------------------+----------------+-------------------+\n",
      "| QKV Preparation (Total) |         0.3507 | -                 | -              | -                 |\n",
      "+-------------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Pure Flex Attention     |         0.3017 | -                 | 3.0607         | -                 |\n",
      "+-------------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Combined (Total)        |         0.64   | -                 | 3.0609         | -                 |\n",
      "+-------------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Overhead                |         0.3384 | -                 | 0.0003         | -                 |\n",
      "+-------------------------+----------------+-------------------+----------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "def sgn(x):\n",
    "    return -1 if x < 0 else (1 if x > 0 else 0)\n",
    "\n",
    "def generate2d(x: int, y: int, ax: int, ay: int, bx: int, by: int, result):\n",
    "    w = abs(ax + ay)\n",
    "    h = abs(bx + by)\n",
    "    dax, day = sgn(ax), sgn(ay)\n",
    "    dbx, dby = sgn(bx), sgn(by)\n",
    "\n",
    "    if h == 1 or w == 1:\n",
    "        if h == 1:\n",
    "            for _ in range(w):\n",
    "                result.append((x, y))\n",
    "                x, y = x + dax, y + day\n",
    "        elif w == 1:\n",
    "            for _ in range(h):\n",
    "                result.append((x, y))\n",
    "                x, y = x + dbx, y + dby\n",
    "        return\n",
    "\n",
    "    ax2, ay2 = ax // 2, ay // 2\n",
    "    bx2, by2 = bx // 2, by // 2\n",
    "    w2 = abs(ax2 + ay2)\n",
    "    h2 = abs(bx2 + by2)\n",
    "\n",
    "    if 2 * w > 3 * h:\n",
    "        if w2 % 2 and w > 2:\n",
    "            ax2, ay2 = ax2 + dax, ay2 + day\n",
    "        generate2d(x, y, ax2, ay2, bx, by, result)\n",
    "        generate2d(x + ax2, y + ay2, ax - ax2, ay - ay2, bx, by, result)\n",
    "    else:\n",
    "        if h2 % 2 and h > 2:\n",
    "            bx2, by2 = bx2 + dbx, by2 + dby\n",
    "        generate2d(x, y, bx2, by2, ax2, ay2, result)\n",
    "        generate2d(x + bx2, y + by2, ax, ay, bx - bx2, by - by2, result)\n",
    "        generate2d(x + (ax - dax) + (bx2 - dbx),\n",
    "                   y + (ay - day) + (by2 - dby),\n",
    "                   -bx2, -by2, -(ax - ax2), -(ay - ay2), result)\n",
    "\n",
    "def gilbert2d(width, height):\n",
    "    result = []\n",
    "    if width >= height:\n",
    "        generate2d(0, 0, width, 0, 0, height, result)\n",
    "    else:\n",
    "        generate2d(0, 0, 0, height, width, 0, result)\n",
    "    return result\n",
    "\n",
    "class GilbertPathCache:\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.device_index_cache = {}\n",
    "\n",
    "    def get_or_create_path(self, H, W):\n",
    "        key = (H, W)\n",
    "        if key not in self.cache:\n",
    "            path = gilbert2d(W, H)\n",
    "\n",
    "            forward_map = torch.zeros((H, W), dtype=torch.long)\n",
    "            reverse_map = torch.zeros((H * W, 2), dtype=torch.long)\n",
    "\n",
    "            for idx, (x, y) in enumerate(path[:H*W]):\n",
    "                if y < H and x < W:\n",
    "                    forward_map[y, x] = idx\n",
    "                    reverse_map[idx, 0] = y\n",
    "                    reverse_map[idx, 1] = x\n",
    "\n",
    "            self.cache[key] = {\n",
    "                'path': path,\n",
    "                'forward_map': forward_map,\n",
    "                'reverse_map': reverse_map,\n",
    "                'y_indices': reverse_map[:, 0].clone(),\n",
    "                'x_indices': reverse_map[:, 1].clone(),\n",
    "                'H': H,\n",
    "                'W': W\n",
    "            }\n",
    "\n",
    "        return self.cache[key]\n",
    "\n",
    "    def get_indices_on_device(self, H, W, device):\n",
    "        device_key = (H, W, str(device))\n",
    "        if device_key in self.device_index_cache:\n",
    "            return self.device_index_cache[device_key]\n",
    "        info = self.get_or_create_path(H, W)\n",
    "        y_dev = info['y_indices'].to(device)\n",
    "        x_dev = info['x_indices'].to(device)\n",
    "        self.device_index_cache[device_key] = (y_dev, x_dev)\n",
    "        return y_dev, x_dev\n",
    "\n",
    "    def precompute_paths(self, resolutions):\n",
    "        for H, W in resolutions:\n",
    "            self.get_or_create_path(H, W)\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self.cache.clear()\n",
    "\n",
    "_global_gilbert_cache = GilbertPathCache()\n",
    "\n",
    "def tensor_to_gilbert_path(x, cache=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: Input tensor, shape (B, H, W, C)\n",
    "        cache: Optional GilbertPathCache instance, use global cache if None\n",
    "    Returns:\n",
    "        Reordered tensor, shape (B, H*W, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    device = x.device\n",
    "    if cache is None:\n",
    "        cache = _global_gilbert_cache\n",
    "\n",
    "    y_indices, x_indices = cache.get_indices_on_device(H, W, device)\n",
    "    gilbert_tensor = x[:, y_indices, x_indices, :]  # (B, H*W, C)\n",
    "\n",
    "    return gilbert_tensor\n",
    "\n",
    "def gilbert_tensor_to_2d(x, H, W, cache=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: Gilbert sequence tensor, shape (B, H*W, C)\n",
    "        H: Target height\n",
    "        W: Target width\n",
    "        cache: Optional GilbertPathCache instance, use global cache if None\n",
    "    Returns:\n",
    "        2D layout tensor, shape (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B, N, C = x.shape\n",
    "    device = x.device\n",
    "\n",
    "    if cache is None:\n",
    "        cache = _global_gilbert_cache\n",
    "\n",
    "    output_2d = torch.zeros((B, H, W, C), dtype=x.dtype, device=device)\n",
    "\n",
    "    valid_n = min(N, H * W)\n",
    "    if valid_n > 0:\n",
    "        y_all, x_all = cache.get_indices_on_device(H, W, device)\n",
    "        y_indices = y_all[:valid_n]\n",
    "        x_indices = x_all[:valid_n]\n",
    "\n",
    "        output_2d[:, y_indices, x_indices, :] = x[:, :valid_n, :]\n",
    "\n",
    "    return output_2d\n",
    "\n",
    "@lru_cache\n",
    "def create_block_mask_cached(score_mod, B, H, M, N, device=\"cuda\",BLOCK_SIZE=128):\n",
    "    block_mask = create_block_mask(score_mod, B, H, M, N, device=device,BLOCK_SIZE=BLOCK_SIZE, _compile=True)\n",
    "    return block_mask\n",
    "\n",
    "def calculate_tflops(flops: float, time_ms: float, multiplier: int) -> float:\n",
    "    return multiplier * flops * (1e3 / time_ms) / 1e12\n",
    "\n",
    "def hilbert_rearrange(x):\n",
    "\n",
    "    x_seq = tensor_to_gilbert_path(x)  # (B, H_img*W_img, C)\n",
    "    return x_seq\n",
    "\n",
    "def hilbert_qkv_projection(x_seq, num_heads: int):\n",
    "    B, S, C = x_seq.shape\n",
    "    head_dim = C // num_heads\n",
    "\n",
    "    if not hasattr(hilbert_qkv_projection, 'qkv_weight'):\n",
    "        hilbert_qkv_projection.qkv_weight = torch.randn(\n",
    "            C, 3*C, device=x_seq.device, dtype=x_seq.dtype\n",
    "        )\n",
    "\n",
    "    qkv = x_seq @ hilbert_qkv_projection.qkv_weight  # (B, H_img*W_img, 3*C)\n",
    "    qkv = qkv.view(B, S, 3, num_heads, head_dim).permute(2, 0, 3, 1, 4)\n",
    "    q, k, v = qkv[0], qkv[1], qkv[2]  # 每个形状: (B, num_heads, H_img*W_img, C//num_heads)\n",
    "\n",
    "    return q, k, v\n",
    "\n",
    "def prepare_hilbert_qkv(x, num_heads):\n",
    "    x_seq = hilbert_rearrange(x)\n",
    "\n",
    "    q, k, v = hilbert_qkv_projection(x_seq, num_heads)\n",
    "\n",
    "    return q, k, v\n",
    "\n",
    "def hilbert_flex_attention_only(q, k, v, score_mod=None, block_mask=None):\n",
    "\n",
    "    return flex_attention(q, k, v, score_mod=score_mod, block_mask=block_mask)\n",
    "\n",
    "def hilbert_window_flex_attention(x, num_heads, score_mod=None, block_mask=None):\n",
    "\n",
    "    x_seq = hilbert_rearrange(x)\n",
    "    q, k, v = hilbert_qkv_projection(x_seq, num_heads)\n",
    "    x_seq = hilbert_flex_attention_only(q, k, v, score_mod=score_mod, block_mask=block_mask)\n",
    "    return x_seq\n",
    "\n",
    "def test_split_performance():\n",
    "    B, H_img, W_img, C, num_heads = 16, 96, 96, 128, 2\n",
    "    WINDOW_SIZE = 121 \n",
    "    S = H_img * W_img\n",
    "\n",
    "    # Prepare input\n",
    "    x = torch.randn(B, H_img, W_img, C, device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "    gradOut = torch.randn(B, H_img, W_img, C, device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "    # Create block mask\n",
    "    def sliding_window_mask(b, h, q_idx, kv_idx):\n",
    "        return (q_idx - kv_idx).abs() <= WINDOW_SIZE // 2\n",
    "    def score_mod_func(score, b, h, q_idx, kv_idx):\n",
    "        rel_pos = (q_idx - kv_idx).to(score.dtype)\n",
    "        return score + rel_pos\n",
    "    block_mask = create_block_mask_cached(sliding_window_mask, 1, 1, S, S, device=x.device, BLOCK_SIZE=128)\n",
    "\n",
    "    # Test Hilbert rearrangement only\n",
    "    hilbert_rearrange_call = lambda: hilbert_rearrange(x)\n",
    "    hilbert_rearrange_ms = do_bench(hilbert_rearrange_call)\n",
    "\n",
    "    # Test QKV projection only\n",
    "    x_seq = hilbert_rearrange_call()\n",
    "    qkv_proj_call = lambda: hilbert_qkv_projection(x_seq, num_heads)\n",
    "    qkv_proj_ms = do_bench(qkv_proj_call)\n",
    "\n",
    "    # Test QKV preparation (combined)\n",
    "    prepare_call = lambda: prepare_hilbert_qkv(x, num_heads)\n",
    "    prepare_ms = do_bench(prepare_call)\n",
    "\n",
    "    # Pre-compute QKV for pure attention test\n",
    "    q, k, v = prepare_call()\n",
    "\n",
    "    # Test pure flex attention only\n",
    "    pure_attention_call = lambda: hilbert_flex_attention_only(q, k, v, score_mod=score_mod_func, block_mask=block_mask)\n",
    "    pure_attention_ms = do_bench(pure_attention_call)\n",
    "\n",
    "    # Test combined function\n",
    "    combined_call = lambda: hilbert_window_flex_attention(x, num_heads, score_mod=score_mod_func, block_mask=block_mask)\n",
    "    combined_ms = do_bench(combined_call)\n",
    "\n",
    "    # Backward test\n",
    "    combined_out = combined_call()\n",
    "    pure_out = pure_attention_call()\n",
    "    gradOut_seq = gradOut.view(B, H_img*W_img, num_heads, C // num_heads).permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "    pure_bw_ms = do_bench(lambda: pure_out.backward(gradOut_seq, retain_graph=True))\n",
    "    combined_bw_ms = do_bench(lambda: combined_out.backward(gradOut_seq, retain_graph=True))\n",
    "\n",
    "    results = [\n",
    "        [\"Hilbert Rearrangement\", f\"{hilbert_rearrange_ms:.4f}\", \"-\", \"-\", \"-\"],\n",
    "        [\"QKV Projection\", f\"{qkv_proj_ms:.4f}\", \"-\", \"-\", \"-\"],\n",
    "        [\"QKV Preparation (Total)\", f\"{prepare_ms:.4f}\", \"-\", \"-\", \"-\"],\n",
    "        [\"Pure Flex Attention\", f\"{pure_attention_ms:.4f}\", \"-\", f\"{pure_bw_ms:.4f}\", \"-\"],\n",
    "        [\"Combined (Total)\", f\"{combined_ms:.4f}\", \"-\", f\"{combined_bw_ms:.4f}\", \"-\"],\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nTest split performance:\")\n",
    "    print(tabulate(results, headers=[\"Operation\", \"FW Time (ms)\", \"BW Time (ms)\"], tablefmt=\"grid\"))\n",
    "\n",
    "    # Clean up\n",
    "    del x, q, k, v, combined_out, pure_out\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "test_split_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a2009",
   "metadata": {
    "id": "6b2a2009"
   },
   "source": [
    "# FlexSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e88fe2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10467,
     "status": "ok",
     "timestamp": 1758422348860,
     "user": {
      "displayName": "Yunge Li",
      "userId": "12625983184086276329"
     },
     "user_tz": 240
    },
    "id": "35e88fe2",
    "outputId": "9e8548cd-3fb9-4c4b-f12d-ceaa6cd37e13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分离函数性能测试:\n",
      "+---------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Operation           |   FW Time (ms) | FW FLOPS (TF/s)   | BW Time (ms)   | BW FLOPS (TF/s)   |\n",
      "+=====================+================+===================+================+===================+\n",
      "| QKV Preparation     |         0.3211 | -                 | -              | -                 |\n",
      "+---------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Pure Flex Attention |         4.123  | -                 | 13.8566        | -                 |\n",
      "+---------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Combined (Total)    |         4.2111 | -                 | 13.6113        | -                 |\n",
      "+---------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Overhead            |         0.0881 | -                 | -0.2453        | -                 |\n",
      "+---------------------+----------------+-------------------+----------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "@lru_cache\n",
    "def create_block_mask_cached(score_mod, B, H, M, N, device=\"cuda\",BLOCK_SIZE=128):\n",
    "    block_mask = create_block_mask(score_mod, B, H, M, N, device=device,BLOCK_SIZE=BLOCK_SIZE, _compile=True)\n",
    "    return block_mask\n",
    "\n",
    "def calculate_tflops(flops: float, time_ms: float, multiplier: int) -> float:\n",
    "    return multiplier * flops * (1e3 / time_ms) / 1e12\n",
    "\n",
    "def prepare_hilbert_qkv(x, num_heads):\n",
    "\n",
    "    B, H_img, W_img, C = x.shape\n",
    "    S = H_img * W_img\n",
    "    x_seq = x.view(B, S, C) \n",
    "\n",
    "    if not hasattr(prepare_hilbert_qkv, 'qkv_weight'):\n",
    "        prepare_hilbert_qkv.qkv_weight = torch.randn(\n",
    "            C, 3*C, device=x_seq.device, dtype=x_seq.dtype\n",
    "        )\n",
    "\n",
    "    # QKV projection\n",
    "    qkv = x_seq @ prepare_hilbert_qkv.qkv_weight  # (B, S, 3*C)\n",
    "    qkv = qkv.view(B, S, 3, num_heads, C // num_heads).permute(2, 0, 3, 1, 4)\n",
    "    q, k, v = qkv[0], qkv[1], qkv[2] \n",
    "\n",
    "    return q, k, v\n",
    "\n",
    "def flex_attention_only(q, k, v, score_mod=None, block_mask=None):\n",
    "\n",
    "    return flex_attention(q, k, v, score_mod=score_mod, block_mask=block_mask)\n",
    "\n",
    "def window_flex_attention(x, num_heads, score_mod=None, block_mask=None):\n",
    "    \"\"\"Original combined function for backward compatibility\"\"\"\n",
    "    q, k, v = prepare_hilbert_qkv(x, num_heads)\n",
    "    x_seq = flex_attention_only(q, k, v, score_mod=score_mod, block_mask=block_mask)\n",
    "    return x_seq\n",
    "\n",
    "# Test performance of split functions\n",
    "def test_split_performance():\n",
    "    B, H_img, W_img, C, num_heads = 16, 128, 128, 128, 2\n",
    "    S = H_img * W_img\n",
    "    WINDOW = 17\n",
    "\n",
    "    # Prepare input\n",
    "    x = torch.randn(B, H_img, W_img, C, device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "    gradOut = torch.randn(B, H_img, W_img, C, device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "    # Create block mask\n",
    "    def sasa_mask(b, h, q_idx, kv_idx):\n",
    "\n",
    "        def get_x_y(idx):\n",
    "            return idx // W_img, idx % W_img\n",
    "\n",
    "        q_x, q_y = get_x_y(q_idx)\n",
    "        kv_x, kv_y = get_x_y(kv_idx)\n",
    "        horizontal_mask = (q_x - kv_x).abs() <= WINDOW // 2\n",
    "        vertical_mask = (q_y - kv_y).abs() <= WINDOW // 2\n",
    "        return horizontal_mask & vertical_mask\n",
    "\n",
    "    def score_mod_func(score, b, h, q_idx, kv_idx):\n",
    "        rel_pos = (q_idx - kv_idx).to(score.dtype)\n",
    "        return score + rel_pos\n",
    "\n",
    "    block_mask = create_block_mask_cached(sasa_mask, 1, 1, S, S, device=x.device, BLOCK_SIZE=128)\n",
    "    # Test QKV preparation only\n",
    "    prepare_call = lambda: prepare_hilbert_qkv(x, num_heads)\n",
    "    prepare_ms = do_bench(prepare_call)\n",
    "\n",
    "    # Pre-compute QKV for pure attention test\n",
    "    q, k, v = prepare_call()\n",
    "\n",
    "    # Test pure flex attention only\n",
    "    pure_attention_call = lambda: flex_attention_only(q, k, v, score_mod=score_mod_func, block_mask=block_mask)\n",
    "    pure_attention_ms = do_bench(pure_attention_call)\n",
    "\n",
    "    # Test combined function\n",
    "    combined_call = lambda: window_flex_attention(x, num_heads,score_mod=score_mod_func, block_mask=block_mask)\n",
    "    combined_ms = do_bench(combined_call)\n",
    "\n",
    "    # Backward test\n",
    "    combined_out = combined_call()\n",
    "    pure_out = pure_attention_call()\n",
    "    gradOut_seq = gradOut.view(B, H_img*W_img, num_heads, C // num_heads).permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "    pure_bw_ms = do_bench(lambda: pure_out.backward(gradOut_seq, retain_graph=True))\n",
    "    combined_bw_ms = do_bench(lambda: combined_out.backward(gradOut_seq, retain_graph=True))\n",
    "\n",
    "    results = [\n",
    "        [\"QKV Preparation\", f\"{prepare_ms:.4f}\", \"-\", \"-\", \"-\"],\n",
    "        [\"Pure Flex Attention\", f\"{pure_attention_ms:.4f}\", \"-\", f\"{pure_bw_ms:.4f}\", \"-\"],\n",
    "        [\"Combined (Total)\", f\"{combined_ms:.4f}\", \"-\", f\"{combined_bw_ms:.4f}\", \"-\"],\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nTest split performance:\")\n",
    "    print(tabulate(results, headers=[\"Operation\", \"FW Time (ms)\", \"BW Time (ms)\"], tablefmt=\"grid\"))\n",
    "\n",
    "    # Clean up\n",
    "    del x, q, k, v, combined_out, pure_out\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "test_split_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea1d8f",
   "metadata": {
    "id": "b7ea1d8f"
   },
   "source": [
    "# NA2D (Flex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c6ce2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2996,
     "status": "ok",
     "timestamp": 1758420614023,
     "user": {
      "displayName": "Yunge Li",
      "userId": "12625983184086276329"
     },
     "user_tz": 240
    },
    "id": "10c6ce2c",
    "outputId": "398b3349-0b06-45ad-f1ae-64538f4f1548"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分离函数性能测试:\n",
      "+---------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Operation           |   FW Time (ms) | FW FLOPS (TF/s)   | BW Time (ms)   | BW FLOPS (TF/s)   |\n",
      "+=====================+================+===================+================+===================+\n",
      "| QKV Preparation     |         0.0769 | -                 | -              | -                 |\n",
      "+---------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Pure Flex Attention |         0.3365 | -                 | 1.5804         | -                 |\n",
      "+---------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Combined (Total)    |         0.4002 | -                 | 1.5789         | -                 |\n",
      "+---------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Overhead            |         0.0637 | -                 | -0.0015        | -                 |\n",
      "+---------------------+----------------+-------------------+----------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "@lru_cache\n",
    "def create_block_mask_cached(score_mod, B, H, M, N, device=\"cuda\",BLOCK_SIZE=128):\n",
    "    block_mask = create_block_mask(score_mod, B, H, M, N, device=device,BLOCK_SIZE=BLOCK_SIZE, _compile=True)\n",
    "    return block_mask\n",
    "\n",
    "def calculate_tflops(flops: float, time_ms: float, multiplier: int) -> float:\n",
    "    return multiplier * flops * (1e3 / time_ms) / 1e12\n",
    "\n",
    "def prepare_hilbert_qkv(x, num_heads):\n",
    "\n",
    "    B, H_img, W_img, C = x.shape\n",
    "    S = H_img * W_img\n",
    "    x_seq = x.view(B, S, C) \n",
    "\n",
    "    if not hasattr(prepare_hilbert_qkv, 'qkv_weight'):\n",
    "        prepare_hilbert_qkv.qkv_weight = torch.randn(\n",
    "            C, 3*C, device=x_seq.device, dtype=x_seq.dtype\n",
    "        )\n",
    "\n",
    "    # QKV projection\n",
    "    qkv = x_seq @ prepare_hilbert_qkv.qkv_weight  # (B, S, 3*C)\n",
    "    qkv = qkv.view(B, S, 3, num_heads, C // num_heads).permute(2, 0, 3, 1, 4)\n",
    "    q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "    return q, k, v\n",
    "\n",
    "def flex_attention_only(q, k, v, score_mod=None, block_mask=None):\n",
    "\n",
    "    return flex_attention(q, k, v, score_mod=score_mod, block_mask=block_mask)\n",
    "\n",
    "def window_flex_attention(x, num_heads, score_mod=None, block_mask=None):\n",
    "\n",
    "    q, k, v = prepare_hilbert_qkv(x, num_heads)\n",
    "    x_seq = flex_attention_only(q, k, v, score_mod=score_mod, block_mask=block_mask)\n",
    "    return x_seq\n",
    "\n",
    "# Test performance of split functions\n",
    "def test_split_performance():\n",
    "    B, H_img, W_img, C, num_heads = 16, 56, 56, 128, 2\n",
    "    S = H_img * W_img\n",
    "\n",
    "    # Prepare input\n",
    "    x = torch.randn(B, H_img, W_img, C, device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "    gradOut = torch.randn(B, H_img, W_img, C, device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "    # Create block mask\n",
    "    K_W = 7\n",
    "    K_H = 7\n",
    "    def get_x_y(idx):\n",
    "        return idx // W_img, idx % W_img\n",
    "\n",
    "\n",
    "    def natten_mask(\n",
    "        b,\n",
    "        h,\n",
    "        q_idx,\n",
    "        kv_idx,\n",
    "    ):\n",
    "        q_x, q_y = get_x_y(q_idx)\n",
    "        kv_x, kv_y = get_x_y(kv_idx)\n",
    "\n",
    "        kernel_x = q_x.clamp(K_W // 2, (W_img - 1) - K_W // 2)\n",
    "        kernel_y = q_y.clamp(K_H // 2, (H_img - 1) - K_H // 2)\n",
    "        hori_mask = (kernel_x - kv_x).abs() <= K_W // 2\n",
    "        vert_mask = (kernel_y - kv_y).abs() <= K_H // 2\n",
    "        return hori_mask & vert_mask\n",
    "\n",
    "    def score_mod_func(score, b, h, q_idx, kv_idx):\n",
    "        rel_pos = (q_idx - kv_idx).to(score.dtype)\n",
    "        return score + rel_pos\n",
    "\n",
    "    block_mask = create_block_mask_cached(natten_mask, 1, 1, S, S, device=x.device, BLOCK_SIZE=128)\n",
    "    # Test QKV preparation only\n",
    "    prepare_call = lambda: prepare_hilbert_qkv(x, num_heads)\n",
    "    prepare_ms = do_bench(prepare_call)\n",
    "\n",
    "    # Pre-compute QKV for pure attention test\n",
    "    q, k, v = prepare_call()\n",
    "\n",
    "    # Test pure flex attention only\n",
    "    pure_attention_call = lambda: flex_attention_only(q, k, v, score_mod=score_mod_func, block_mask=block_mask)\n",
    "    pure_attention_ms = do_bench(pure_attention_call)\n",
    "\n",
    "    # Test combined function\n",
    "    combined_call = lambda: window_flex_attention(x, num_heads,score_mod=score_mod_func, block_mask=block_mask)\n",
    "    combined_ms = do_bench(combined_call)\n",
    "\n",
    "    # Backward test\n",
    "    combined_out = combined_call()\n",
    "    pure_out = pure_attention_call()\n",
    "    gradOut_seq = gradOut.view(B, H_img*W_img, num_heads, C // num_heads).permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "    pure_bw_ms = do_bench(lambda: pure_out.backward(gradOut_seq, retain_graph=True))\n",
    "    combined_bw_ms = do_bench(lambda: combined_out.backward(gradOut_seq, retain_graph=True))\n",
    "\n",
    "    results = [\n",
    "        [\"QKV Preparation\", f\"{prepare_ms:.4f}\", \"-\", \"-\", \"-\"],\n",
    "        [\"Pure Flex Attention\", f\"{pure_attention_ms:.4f}\", \"-\", f\"{pure_bw_ms:.4f}\", \"-\"],\n",
    "        [\"Combined (Total)\", f\"{combined_ms:.4f}\", \"-\", f\"{combined_bw_ms:.4f}\", \"-\"],\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nTest split performance:\")\n",
    "    print(tabulate(results, headers=[\"Operation\", \"FW Time (ms)\", \"BW Time (ms)\"], tablefmt=\"grid\"))\n",
    "\n",
    "    # Clean up\n",
    "    del x, q, k, v, combined_out, pure_out\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "test_split_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c771afe",
   "metadata": {
    "id": "4c771afe"
   },
   "source": [
    "# HNA (Flex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5252a89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3225,
     "status": "ok",
     "timestamp": 1758420437303,
     "user": {
      "displayName": "Yunge Li",
      "userId": "12625983184086276329"
     },
     "user_tz": 240
    },
    "id": "c5252a89",
    "outputId": "4eac8c70-d356-47a2-e1e5-b998860048f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分离函数性能测试:\n",
      "+-------------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Operation               |   FW Time (ms) | FW FLOPS (TF/s)   | BW Time (ms)   | BW FLOPS (TF/s)   |\n",
      "+=========================+================+===================+================+===================+\n",
      "| Hilbert Rearrangement   |         0.0896 | -                 | -              | -                 |\n",
      "+-------------------------+----------------+-------------------+----------------+-------------------+\n",
      "| QKV Projection          |         0.0776 | -                 | -              | -                 |\n",
      "+-------------------------+----------------+-------------------+----------------+-------------------+\n",
      "| QKV Preparation (Total) |         0.1298 | -                 | -              | -                 |\n",
      "+-------------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Pure Flex Attention     |         0.2625 | -                 | 1.5588         | -                 |\n",
      "+-------------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Combined (Total)        |         0.4499 | -                 | 1.5590         | -                 |\n",
      "+-------------------------+----------------+-------------------+----------------+-------------------+\n",
      "| Overhead                |         0.1874 | -                 | 0.0002         | -                 |\n",
      "+-------------------------+----------------+-------------------+----------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "def sgn(x):\n",
    "    return -1 if x < 0 else (1 if x > 0 else 0)\n",
    "\n",
    "def generate2d(x: int, y: int, ax: int, ay: int, bx: int, by: int, result):\n",
    "    w = abs(ax + ay)\n",
    "    h = abs(bx + by)\n",
    "    dax, day = sgn(ax), sgn(ay)\n",
    "    dbx, dby = sgn(bx), sgn(by)\n",
    "\n",
    "    if h == 1 or w == 1:\n",
    "        if h == 1:\n",
    "            for _ in range(w):\n",
    "                result.append((x, y))\n",
    "                x, y = x + dax, y + day\n",
    "        elif w == 1:\n",
    "            for _ in range(h):\n",
    "                result.append((x, y))\n",
    "                x, y = x + dbx, y + dby\n",
    "        return\n",
    "\n",
    "    ax2, ay2 = ax // 2, ay // 2\n",
    "    bx2, by2 = bx // 2, by // 2\n",
    "    w2 = abs(ax2 + ay2)\n",
    "    h2 = abs(bx2 + by2)\n",
    "\n",
    "    if 2 * w > 3 * h:\n",
    "        if w2 % 2 and w > 2:\n",
    "            ax2, ay2 = ax2 + dax, ay2 + day\n",
    "        generate2d(x, y, ax2, ay2, bx, by, result)\n",
    "        generate2d(x + ax2, y + ay2, ax - ax2, ay - ay2, bx, by, result)\n",
    "    else:\n",
    "        if h2 % 2 and h > 2:\n",
    "            bx2, by2 = bx2 + dbx, by2 + dby\n",
    "        generate2d(x, y, bx2, by2, ax2, ay2, result)\n",
    "        generate2d(x + bx2, y + by2, ax, ay, bx - bx2, by - by2, result)\n",
    "        generate2d(x + (ax - dax) + (bx2 - dbx),\n",
    "                   y + (ay - day) + (by2 - dby),\n",
    "                   -bx2, -by2, -(ax - ax2), -(ay - ay2), result)\n",
    "\n",
    "def gilbert2d(width, height):\n",
    "    result = []\n",
    "    if width >= height:\n",
    "        generate2d(0, 0, width, 0, 0, height, result)\n",
    "    else:\n",
    "        generate2d(0, 0, 0, height, width, 0, result)\n",
    "    return result\n",
    "\n",
    "class GilbertPathCache:\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.device_index_cache = {}\n",
    "\n",
    "    def get_or_create_path(self, H, W):\n",
    "        key = (H, W)\n",
    "        if key not in self.cache:\n",
    "            path = gilbert2d(W, H)\n",
    "\n",
    "            forward_map = torch.zeros((H, W), dtype=torch.long)\n",
    "            reverse_map = torch.zeros((H * W, 2), dtype=torch.long)\n",
    "\n",
    "            for idx, (x, y) in enumerate(path[:H*W]):\n",
    "                if y < H and x < W:\n",
    "                    forward_map[y, x] = idx\n",
    "                    reverse_map[idx, 0] = y\n",
    "                    reverse_map[idx, 1] = x\n",
    "\n",
    "            self.cache[key] = {\n",
    "                'path': path,\n",
    "                'forward_map': forward_map,\n",
    "                'reverse_map': reverse_map,\n",
    "                'y_indices': reverse_map[:, 0].clone(),\n",
    "                'x_indices': reverse_map[:, 1].clone(),\n",
    "                'H': H,\n",
    "                'W': W\n",
    "            }\n",
    "\n",
    "        return self.cache[key]\n",
    "\n",
    "    def get_indices_on_device(self, H, W, device):\n",
    "        device_key = (H, W, str(device))\n",
    "        if device_key in self.device_index_cache:\n",
    "            return self.device_index_cache[device_key]\n",
    "        info = self.get_or_create_path(H, W)\n",
    "        y_dev = info['y_indices'].to(device)\n",
    "        x_dev = info['x_indices'].to(device)\n",
    "        self.device_index_cache[device_key] = (y_dev, x_dev)\n",
    "        return y_dev, x_dev\n",
    "\n",
    "    def precompute_paths(self, resolutions):\n",
    "        for H, W in resolutions:\n",
    "            self.get_or_create_path(H, W)\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self.cache.clear()\n",
    "\n",
    "_global_gilbert_cache = GilbertPathCache()\n",
    "\n",
    "def tensor_to_gilbert_path(x, cache=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: Input tensor, shape (B, H, W, C)\n",
    "        cache: Optional GilbertPathCache instance, use global cache if None\n",
    "    Returns:\n",
    "        Reordered tensor, shape (B, H*W, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    device = x.device\n",
    "    if cache is None:\n",
    "        cache = _global_gilbert_cache\n",
    "\n",
    "    y_indices, x_indices = cache.get_indices_on_device(H, W, device)\n",
    "    gilbert_tensor = x[:, y_indices, x_indices, :]  # (B, H*W, C)\n",
    "\n",
    "    return gilbert_tensor\n",
    "\n",
    "def gilbert_tensor_to_2d(x, H, W, cache=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: Gilbert sequence tensor, shape (B, H*W, C)\n",
    "        H: Target height\n",
    "        W: Target width\n",
    "        cache: Optional GilbertPathCache instance, use global cache if None\n",
    "    Returns:\n",
    "        2D layout tensor, shape (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B, N, C = x.shape\n",
    "    device = x.device\n",
    "\n",
    "    if cache is None:\n",
    "        cache = _global_gilbert_cache\n",
    "\n",
    "    output_2d = torch.zeros((B, H, W, C), dtype=x.dtype, device=device)\n",
    "\n",
    "    valid_n = min(N, H * W)\n",
    "    if valid_n > 0:\n",
    "        y_all, x_all = cache.get_indices_on_device(H, W, device)\n",
    "        y_indices = y_all[:valid_n]\n",
    "        x_indices = x_all[:valid_n]\n",
    "\n",
    "        output_2d[:, y_indices, x_indices, :] = x[:, :valid_n, :]\n",
    "\n",
    "    return output_2d\n",
    "\n",
    "@lru_cache\n",
    "def create_block_mask_cached(score_mod, B, H, M, N, device=\"cuda\",BLOCK_SIZE=128):\n",
    "    block_mask = create_block_mask(score_mod, B, H, M, N, device=device,BLOCK_SIZE=BLOCK_SIZE, _compile=True)\n",
    "    return block_mask\n",
    "\n",
    "def calculate_tflops(flops: float, time_ms: float, multiplier: int) -> float:\n",
    "    return multiplier * flops * (1e3 / time_ms) / 1e12\n",
    "\n",
    "def hilbert_rearrange(x):\n",
    "\n",
    "    x_seq = tensor_to_gilbert_path(x)  # (B, H_img*W_img, C)\n",
    "    return x_seq\n",
    "\n",
    "def hilbert_qkv_projection(x_seq, num_heads: int):\n",
    "\n",
    "    B, S, C = x_seq.shape\n",
    "    head_dim = C // num_heads\n",
    "\n",
    "    if not hasattr(hilbert_qkv_projection, 'qkv_weight'):\n",
    "        hilbert_qkv_projection.qkv_weight = torch.randn(\n",
    "            C, 3*C, device=x_seq.device, dtype=x_seq.dtype\n",
    "        )\n",
    "\n",
    "    # QKV projection\n",
    "    qkv = x_seq @ hilbert_qkv_projection.qkv_weight  # (B, H_img*W_img, 3*C)\n",
    "    qkv = qkv.view(B, S, 3, num_heads, head_dim).permute(2, 0, 3, 1, 4)\n",
    "    q, k, v = qkv[0], qkv[1], qkv[2]  # 每个形状: (B, num_heads, H_img*W_img, C//num_heads)\n",
    "\n",
    "    return q, k, v\n",
    "\n",
    "def prepare_hilbert_qkv(x, num_heads):\n",
    "    \"\"\"Prepare QKV tensors from 2D input using Hilbert curve ordering\"\"\"\n",
    "    x_seq = hilbert_rearrange(x)\n",
    "\n",
    "    q, k, v = hilbert_qkv_projection(x_seq, num_heads)\n",
    "\n",
    "    return q, k, v\n",
    "\n",
    "def hilbert_flex_attention_only(q, k, v, score_mod=None, block_mask=None):\n",
    "\n",
    "    return flex_attention(q, k, v, score_mod=score_mod, block_mask=block_mask)\n",
    "\n",
    "def hilbert_window_flex_attention(x, num_heads, score_mod=None, block_mask=None):\n",
    "    x_seq = hilbert_rearrange(x)\n",
    "    q, k, v = hilbert_qkv_projection(x_seq, num_heads)\n",
    "    x_seq = hilbert_flex_attention_only(q, k, v, score_mod=score_mod, block_mask=block_mask)\n",
    "    return x_seq\n",
    "\n",
    "# Test performance of split functions\n",
    "def test_split_performance():\n",
    "    B, H_img, W_img, C, num_heads = 16, 56, 56, 128, 2\n",
    "    BLOCK = 49 \n",
    "    S = H_img * W_img\n",
    "\n",
    "    # Prepare input\n",
    "    x = torch.randn(B, H_img, W_img, C, device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "    gradOut = torch.randn(B, H_img, W_img, C, device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "    # Create block mask\n",
    "    def natten_1d_mask(b, h, q_idx, kv_idx, KERNEL_SIZE=289, SEQ_LEN=16384):\n",
    "        kernel_center = q_idx.clamp(KERNEL_SIZE // 2, SEQ_LEN - 1 - KERNEL_SIZE // 2)\n",
    "        return (kernel_center - kv_idx).abs() <= KERNEL_SIZE // 2 \n",
    "    def score_mod_func(score, b, h, q_idx, kv_idx):\n",
    "        rel_pos = (q_idx - kv_idx).to(score.dtype)\n",
    "        return score + rel_pos\n",
    "    block_mask = create_block_mask_cached(natten_1d_mask, 1, 1, S, S, device=x.device, BLOCK_SIZE=128)\n",
    "\n",
    "    # Test Hilbert rearrangement only\n",
    "    hilbert_rearrange_call = lambda: hilbert_rearrange(x)\n",
    "    hilbert_rearrange_ms = do_bench(hilbert_rearrange_call)\n",
    "\n",
    "    # Test QKV projection only\n",
    "    x_seq = hilbert_rearrange_call()\n",
    "    qkv_proj_call = lambda: hilbert_qkv_projection(x_seq, num_heads)\n",
    "    qkv_proj_ms = do_bench(qkv_proj_call)\n",
    "\n",
    "    # Test QKV preparation (combined)\n",
    "    prepare_call = lambda: prepare_hilbert_qkv(x, num_heads)\n",
    "    prepare_ms = do_bench(prepare_call)\n",
    "\n",
    "    # Pre-compute QKV for pure attention test\n",
    "    q, k, v = prepare_call()\n",
    "\n",
    "    # Test pure flex attention only\n",
    "    pure_attention_call = lambda: hilbert_flex_attention_only(q, k, v, score_mod=score_mod_func, block_mask=block_mask)\n",
    "    pure_attention_ms = do_bench(pure_attention_call)\n",
    "\n",
    "    # Test combined function\n",
    "    combined_call = lambda: hilbert_window_flex_attention(x, num_heads, score_mod=score_mod_func, block_mask=block_mask)\n",
    "    combined_ms = do_bench(combined_call)\n",
    "\n",
    "    # Backward test\n",
    "    combined_out = combined_call()\n",
    "    pure_out = pure_attention_call()\n",
    "    gradOut_seq = gradOut.view(B, H_img*W_img, num_heads, C // num_heads).permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "    pure_bw_ms = do_bench(lambda: pure_out.backward(gradOut_seq, retain_graph=True))\n",
    "    combined_bw_ms = do_bench(lambda: combined_out.backward(gradOut_seq, retain_graph=True))\n",
    "\n",
    "    results = [\n",
    "        [\"Hilbert Rearrangement\", f\"{hilbert_rearrange_ms:.4f}\", \"-\", \"-\", \"-\"],\n",
    "        [\"QKV Projection\", f\"{qkv_proj_ms:.4f}\", \"-\", \"-\", \"-\"],\n",
    "        [\"QKV Preparation (Total)\", f\"{prepare_ms:.4f}\", \"-\", \"-\", \"-\"],\n",
    "        [\"Pure Flex Attention\", f\"{pure_attention_ms:.4f}\", \"-\", f\"{pure_bw_ms:.4f}\", \"-\"],\n",
    "        [\"Combined (Total)\", f\"{combined_ms:.4f}\", \"-\", f\"{combined_bw_ms:.4f}\", \"-\"],\n",
    "]\n",
    "\n",
    "    print(f\"\\nTest split performance:\")\n",
    "    print(tabulate(results, headers=[\"Operation\", \"FW Time (ms)\", \"BW Time (ms)\"], tablefmt=\"grid\"))\n",
    "\n",
    "    # Clean up\n",
    "    del x, q, k, v, combined_out, pure_out\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "test_split_performance()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
